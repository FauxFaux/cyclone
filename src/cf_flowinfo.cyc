/* Utilities for control flow analysis.
   Copyright (C) 2001 Dan Grossman, Greg Morrisett
   This file is part of the Cyclone compiler.

   The Cyclone compiler is free software; you can redistribute it
   and/or modify it under the terms of the GNU General Public License
   as published by the Free Software Foundation; either version 2 of
   the License, or (at your option) any later version.

   The Cyclone compiler is distributed in the hope that it will be
   useful, but WITHOUT ANY WARRANTY; without even the implied warranty
   of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with the Cyclone compiler; see the file COPYING. If not,
   write to the Free Software Foundation, Inc., 59 Temple Place -
   Suite 330, Boston, MA 02111-1307, USA. */
#include <core.h>
#include <list.h>
#include <set.h>
#include <dict.h>
#include <string.h>
#include "absyn.h"
#include "tcutil.h"
#include "absynpp.h"

#define CF_FLOWINFO_CYC
#include "cf_flowinfo.h"
using Absyn;
using List;
using Absynpp;
namespace CfFlowInfo;

////////////////////////// Shared Constants //////////////////////////
static tunion AbsRVal.UnknownR unknown_none_v = UnknownR(NoneIL);
static tunion AbsRVal.UnknownR unknown_this_v = UnknownR(ThisIL);
static tunion AbsRVal.UnknownR unknown_all_v  = UnknownR(AllIL);
static tunion AbsRVal.Esc      esc_none_v     = Esc(NoneIL);
static tunion AbsRVal.Esc      esc_this_v     = Esc(ThisIL);
static tunion AbsRVal.Esc      esc_all_v      = Esc(AllIL);
absRval_t unknown_none = &unknown_none_v;
absRval_t unknown_this = &unknown_this_v;
absRval_t unknown_all  = &unknown_all_v;
absRval_t esc_none     = &esc_none_v;
absRval_t esc_this     = &esc_this_v;
absRval_t esc_all      = &esc_all_v;

place_set_t mt_place_set() {
  static place_set_t * mt_place_set_opt = NULL;

  if(mt_place_set_opt == NULL)
    mt_place_set_opt = new Dict::empty(place_cmp);
  return *mt_place_set_opt;
}

// updates the given set if not NULL.  Returns true if the place
// is already in the set.
bool update_place_set(place_set_t *set, place_t<`H,`H> place,
		      Position::seg_t loc) {
  if (set != NULL) {
    // see if there's already an entry here
    if (Dict::member(*set,place))
      return true;
    else
      *set = Dict::insert(*set, place, loc);
  }
  return false;
}

static void unique_err(place_t place,
		       string_t err_msg1, string_t err_msg2,
		       Position::seg_t consumed_loc,
		       Position::seg_t loc) {  
  let &Place(root,_) = place;
  switch (root) {
  case &VarRoot(vd):
    if (!Position::segment_equals(consumed_loc,loc)) {
      let s = Position::string_of_segment(consumed_loc);
      Tcutil::terr(loc,err_msg1,Absynpp::qvar2string(vd->name), s);
    }
    else
      Tcutil::terr(loc,err_msg2,Absynpp::qvar2string(vd->name));
    break;
  default:
    Tcutil::impos("error locations not for VarRoots");
  }
}

// checks for conflicts when merging consumed sets
static Position::seg_t combine_consume_f(bool isErr,
					 place_t<`H,`H> place,
					 Position::seg_t loc1,
					 Position::seg_t loc2) {
  if (isErr)
    unique_err(place, "May consume unique pointer %s more than once (cf. %s)",
	       "May consume unique pointer %s more than once", loc1, loc2);
  return loc1;
}

// updates the given set if not NULL.  Returns true if the place
// is already in the set.
place_set_t union_place_set(place_set_t s1, place_set_t s2, bool disjoint) {
//    DEBUG_PRINT("**union_place_set:\n");
//    DEBUG_PRINT("  s1= ");
//    print_place_set(s1);
//    DEBUG_PRINT("\n  s2= ");
//    print_place_set(s2);
  let res = Dict::union_two_c(combine_consume_f,disjoint,s1,s2);
//    DEBUG_PRINT("\n  res= ");
//    print_place_set(res);
//    DEBUG_PRINT("\n");
  return res;
}

void print_place(place_t p);
bool place_set_subset(place_set_t s1, place_set_t s2) {
  if (s1 == s2) return true;
  try {
    region r { 
      let iter = Dict::make_iter(r,s1);
      let elem = *Dict::rchoose(r,s1);
      while(Iter::next(iter,&elem)) {
	let place = elem[0];
	if (!Dict::member(s2,place)) {
	  DEBUG_PRINT("could not find ");
	  DEBUG_PRINT_F(print_place,place);
	  DEBUG_PRINT("  in s2\n");
	  return false;
	}
      }
      return true;
    }
  } catch {
  case Absent: // only if rchoose fails, which means s1 is empty
    return true;
  }
}

bool place_set_equals(place_set_t s1, place_set_t s2) {
//    let sz1 = Dict::cardinality(s1);
//    let sz2 = Dict::cardinality(s2);
//    if (sz1 == sz2) {
//      let d = Dict::intersect_c(combine_consume_f,false,s1,s2);
//      return (Dict::cardinality(d) == sz1);
//    }
//    else return false;
  return place_set_subset(s1,s2) && place_set_subset(s2,s1);
}

//////////////////////////// Utilities ///////////////////////////////
int root_cmp(root_t r1, root_t r2) {
  if(((int)r1) == ((int)r2))
    return 0;
  switch ($(r1,r2)) {
  case $(&VarRoot(vd1),&VarRoot(vd2)): return vd1 - vd2;
  case $(&VarRoot(_),_): return -1;
  case $(_,&VarRoot(_)): return 1;
  case $(&MallocPt(e1,_),&MallocPt(e2,_)): return e1 - e2;
  case $(&MallocPt(_,_),_): return -1;
  case $(_,&MallocPt(_,_)): return 1;
  case $(&InitParam(i1,_), &InitParam(i2,_)): return i1-i2;
  }
}
int place_cmp(place_t p1, place_t p2) {
  if(((int)p1) == ((int)p2))
    return 0;
  int i = root_cmp(p1->root,p2->root);
  if(i != 0)
    return i;
  return list_cmp(strptrcmp,p1->fields,p2->fields);
}

// for debugging
static mstringptr_t place2string(place_t p) {
  list_t<mstringptr_t> sl = NULL;
  switch(p->root) {
  case &VarRoot(vd): sl = new List(new aprintf("%s",*(*vd->name)[1]),sl); break;
  case &MallocPt(e,_):  sl = new List(new aprintf("mpt%d",(int)e),sl); break;
  case &InitParam(i,_): sl = new List(new aprintf("param%d",i),sl); break;
  }
  for(_ fields = p->fields; fields != NULL; fields = fields->tl)
    sl = new List(new aprintf("%s", *fields->hd), sl);
  let ans = new aprintf("%s","");
  for(; sl != NULL; sl = sl->tl)
    *ans = aprintf("%s.%s", *sl->hd, *ans);
  return ans;
}

// Note: We don't have to worry about instantiating polymorphic struct fields
//       b/c they can't be instantiated with aggregates.
// BUT that means fields instantiated with int still must be initialized.
// Sorry, but it's a pain.
// Also, not sure why enums aren't bits_only.
//   (If we ever have memory kinds, then we either need to figure out field's
//    instantiated types or not allow partial initialization of such fields.)
// Note: We could memoize answers here.
static absRval_t i_typ_to_absrval(bool allow_zeroterm,
				  type_t t, absRval_t leafval) {
  if(is_union_type(t))
    return unknown_all;

  switch(Tcutil::compress(t)) {
  case &TunionFieldType(TunionFieldInfo(&KnownTunionfield(_,tuf),_)):
    if(tuf->typs == NULL)
      return leafval;
    fallthru(tuf->typs);
  case &TupleType(tqts):
    aggrdict_t d = Dict::empty(strptrcmp);
    for(int i=0; tqts != NULL; tqts = tqts->tl, ++i)
      d = Dict::insert(d,fieldname(i),
		       i_typ_to_absrval(false,(*tqts->hd)[1],leafval));
    return new Aggregate(d);

  case &AggrType(AggrInfo(info,_)):
    let ad = get_known_aggrdecl(info);
    if(ad->impl==NULL)
      break;
    fallthru(ad->impl->fields);
  case &AnonAggrType(StructA,fs):
    aggrdict_t d = Dict::empty(strptrcmp);
    for(; fs != NULL; fs = fs->tl) {
      let &Aggrfield(n,_,t2,_,_) = fs->hd;
      if((*n).size != 1) //empty name means bit field so no need to include it
	d = Dict::insert(d, n, i_typ_to_absrval(false, t2,leafval));
    }
    return new Aggregate(d);
  case &ArrayType(ArrayInfo{et,_,_,zeroterm,_}) && conref_def(false,zeroterm):
    // special case for zero-terminated arrays of bits-only things -- 
    // here, we initialize them at translation by putting a 0 in at the end.
    return (allow_zeroterm && Tcutil::bits_only(et)) ? unknown_all : leafval;
  case &TagType(t):
    // KLUDGE: if a singleton is initialized, we know the value
    // Note: It is unsound to use TagCmps if the value is uninitialized!
    switch(leafval) {
    case &UnknownR(AllIL):
    case &Esc(AllIL):
    case Zero:
    case NotZeroAll: return new TagCmps(list(new TagCmp(Eq,t)));
    default: return leafval;
    }
    case &PointerType(PtrInfo{_,_,PtrAtts{_,nbl,_,_,_}}) 
      && !conref_def(false,nbl):
	switch(leafval) {
	case &UnknownR(ThisIL): return NotZeroThis;
	case &UnknownR(AllIL): return NotZeroAll;
	default: break;
	}
    break;
  default: break;
  }
  
  return Tcutil::bits_only(t) ? unknown_all : leafval;
}

absRval_t typ_to_absrval(type_t t, absRval_t leafval) {
  return i_typ_to_absrval(true, t, leafval);
}

static bool prefix_of_member(region_t<`r> r, place_t<`r2,`r3> place,
			     Dict::dict_t<place_t<`r2,`r3>,Position::seg_t,`H> set) {
  let elem = $(place,NULL);
  let iter = Dict::make_iter(r,set);
  while(Iter::next(iter,&elem)) {
    let place2 = elem[0];
    if(root_cmp(place->root, place2->root) != 0)
      continue;
    let fs1 = place->fields;
    let fs2 = place2->fields;
    for(; fs1 != NULL && fs2 != NULL; fs1 = fs1->tl, fs2 = fs2->tl)
      if(strptrcmp(fs1->hd,fs2->hd) != 0)
	break;
    if(fs1 == NULL)
      return true;
  }
  return false;
}

//////////////////////// Managing Escapes ///////////////////////////

// Note: This would be a great place to delay an idempotent genregion
//       because we often won't have any places! (Also would want to pick
//       a very small initial region size.)
static struct EscPile<`r::R> {
  region_t<`r>       rgn;
  list_t<place_t,`r> places;
};
typedef struct EscPile<`r1> @`r2 escpile_t<`r1,`r2>;

static void add_place(escpile_t pile, place_t<`H,`H> place) {
  // we expect lots of inserts and short lists, so we don't bother to sort
  // we check for repeats to avoid a very unlikely exponential blowup
  if(!List::mem(place_cmp, pile->places, place))
    pile->places = rnew(pile->rgn) List(place, pile->places);
}
static void add_places(escpile_t pile, `a a, absRval_t r) {
  switch(r) {
  case &AddressOf(p): add_place(pile, p); return;
  case &Aggregate(d): Dict::iter_c(add_places, pile, d); return;
  default: return;
  }
}

// WARNING: do not use this to clobber with an Esc unless that's the point!
static absRval_t insert_place_inner(absRval_t new_val,absRval_t old_val) {
  switch(old_val) {
  case &Aggregate(d): return new Aggregate(Dict::map_c(insert_place_inner,
						       new_val, d));
  default: return new_val;
  }
}
static absRval_t insert_place_outer(list_t<field_name_t> fs,
				    absRval_t old_val, absRval_t new_val) {
  if(fs==NULL) 
    return insert_place_inner(new_val,old_val);
  switch($(fs,old_val)) {
  case $(&List(hd,tl),&Aggregate(d)):
    let new_child = insert_place_outer(tl,Dict::lookup(d,hd), new_val);
    return new Aggregate(Dict::insert(d, hd, new_child));
  default: throw new Core::Impossible("bad insert place");
  }
}
// Note: this terminates because we thread the dictionary.
// Note: we need to catch not in the dict b/c of how join_flow uses this code?
// Note: we should only be escaping leaves now -- never Aggregate!
// Note: There's a quadratic behavior here that won't be a problem in practice.
static flowdict_t escape_these(escpile_t pile, place_set_t * all_changed,
			       flowdict_t d) {
  while(pile->places != NULL) {
    let place = pile->places->hd;
    pile->places = pile->places->tl;
    update_place_set(all_changed,place,NULL);
    absRval_t oldval, newval;
    try oldval = lookup_place(d,place); 
    catch { case Dict::Absent: continue; }
    switch(initlevel(d,oldval)) {
    case AllIL:  newval = esc_all;  break;
    case ThisIL: newval = esc_this; break;
    case NoneIL: newval = esc_none; break;
    }
    add_places(pile, 0, oldval);
    d = Dict::insert(d,place->root,
		     insert_place_outer(place->fields,
					Dict::lookup(d,place->root),
					newval));
  }
  return d;
}

////////////////////////// Flow Gets and (functional) Sets ///////////////////

// We must detect cycles in the MustPointTo graph or we won't terminate.
// We heap-allocate the seen list b/c we expect it to be very very short.
static struct InitlevelEnv {
  flowdict_t d;
  list_t<place_t> seen;
};
static initlevel_t initlevel_approx(absRval_t r) {
  switch(r) {
  case &UnknownR(il): return il;
  case &Esc(il):      return il;
  case Zero:
  case NotZeroAll:    return AllIL;
  case NotZeroThis:   return ThisIL;
    // KLUDGE: Maybe r is actually uninitialized, but then whatever test
    //  gave it TagCmps would fail.  In fact, that could only happen for
    //  a tag_t<`a> place because ints are always considered initialized
  case &TagCmps(_):   return AllIL;
  default: throw new Core::Impossible("initlevel_approx");
  }
}
static initlevel_t initlevel_rec(struct InitlevelEnv @ env, `a a, 
				 absRval_t r, initlevel_t acc) {
  initlevel_t this_ans;
  if(acc == NoneIL) return NoneIL;
  switch(r) {
  case &Aggregate(d): this_ans = Dict::fold_c(initlevel_rec,env,d,AllIL); break;
  case &AddressOf(p): 
    if(List::mem(place_cmp, env->seen, p))
      this_ans = AllIL;
    else {
      env->seen = new List(p, env->seen);
      this_ans  = initlevel_rec(env,0,lookup_place(env->d,p),AllIL);
      env->seen = env->seen->tl;
      if(this_ans == NoneIL)
	this_ans = ThisIL;
    }
    break;
  default: this_ans = initlevel_approx(r);
  }
  if(this_ans == NoneIL) return NoneIL;
  if(this_ans == ThisIL || acc == ThisIL) return ThisIL;
  return AllIL;
}
initlevel_t initlevel(flowdict_t d, absRval_t r) {
  let env = InitlevelEnv(d,NULL);
  return initlevel_rec(&env,0,r,AllIL);
}

absRval_t lookup_place(flowdict_t d, place_t place) {
  let &Place(root,fields) = place;
  let ans = Dict::lookup(d,root);
  for(; fields != NULL; fields = fields->tl)
    switch($(ans,fields->hd)) {
    case $(&Aggregate(d2), fname): 
      ans = Dict::lookup(d2,fname); 
      break;
    default: throw new Core::Impossible("bad lookup_place");
    }
  return ans;
}

static bool is_rval_unescaped(`a a, `b b, absRval_t rval) {
  switch(rval) {
  case &Esc(_):       return false;
  case &Aggregate(d): return Dict::forall_c(is_rval_unescaped,0,d);
  default:            return true;
  }
}
bool is_unescaped(flowdict_t d, place_t place) {
  return is_rval_unescaped(0,0,lookup_place(d,place));
}

// only things pointed to escape
flowdict_t escape_deref(flowdict_t d, place_set_t * all_changed, absRval_t r) {
  region rgn {
    escpile_t pile = rnew(rgn) EscPile(rgn,NULL);
    add_places(pile, 0, r);
    return escape_these(pile, all_changed, d);
  }
}

static struct AssignEnv<`r::R> {
  escpile_t<`r,`r> pile;
  flowdict_t       d;
  Position::seg_t  loc;
};
static absRval_t assign_place_inner(struct AssignEnv @ env, `a a,
				    absRval_t oldval, absRval_t newval) {
  // assignments to escaped places must be fully-init because other aliases
  // might assume fully-init.  Always causes place to be added for same reason.
  switch($(oldval,newval)) {
  case $(&Esc(_), &AddressOf(p)): add_place(env->pile,p); fallthru;
  case $(&Esc(_), _):
    if(initlevel(env->d,newval) != AllIL)
      Tcutil::terr(env->loc, "assignment puts possibly-uninitialized data in "
		   "an escaped location");
    return esc_all;
  case $(&Aggregate(d1), &Aggregate(d2)):
    // re-ordering dicts may reduce allocation?
    let new_d = Dict::union_two_c(assign_place_inner, env, d1, d2);
    if(new_d == d1) return oldval;
    if(new_d == d2) return newval;
    return new Aggregate(new_d);
  case $(_, &Esc(il)): // we already know we don't have an escaped location
    switch(il) {
    case NoneIL: return unknown_none;
    case ThisIL: return unknown_this;
    case AllIL:  return unknown_all;
    }
  default: return newval;
  }
}
static absRval_t assign_place_outer(struct AssignEnv @ env, 
				    list_t<field_name_t> fs,
				    absRval_t oldval, absRval_t newval) {
  if(fs == NULL) return assign_place_inner(env,0,oldval,newval);
  switch($(fs,oldval)) {
  case $(&List(hd,tl),&Aggregate(d)):
    let new_child = assign_place_outer(env,tl,Dict::lookup(d,hd),newval);
    return new Aggregate(Dict::insert(d, hd, new_child));
  default: throw new Core::Impossible("bad assign place");
  }
}
flowdict_t assign_place(Position::seg_t loc, flowdict_t d,
			place_set_t * all_changed, place_t<`H,`H> place, 
			absRval_t r) {
  update_place_set(all_changed,place,NULL);
  region rgn {
    let &Place(root,fs) = place;
    struct AssignEnv<`rgn> env = AssignEnv(rnew(rgn) EscPile(rgn,NULL), d, loc);
    absRval_t newval= assign_place_outer(&env,fs,Dict::lookup(d,root),r);
    return escape_these(env.pile, all_changed, Dict::insert(d, root, newval));
  }
}

/////////////////// Join, After, and Lessthan ///////////////////////////////
static struct JoinEnv<`r::R> {
  escpile_t<`r,`r> pile;
  flowdict_t       d1;
  flowdict_t       d2;
};
typedef struct JoinEnv<`r1> @`r2 joinenv_t<`r1,`r2>;
static enum WhoIsChanged { Neither, One, Two }; // if both changed, take join
static struct AfterEnv<`r::R> {
  struct JoinEnv<`r> joinenv;
  place_set_t chg1; // should region-allocate!
  place_set_t chg2; // should region-allocate!
  place_t curr_place; // a pain to stack-allocate, but should!
  list_t<field_name_t> @ last_field_cell;
  enum WhoIsChanged changed;
};
typedef struct AfterEnv<`r1> @`r2 afterenv_t<`r1,`r2>;

static bool absRval_lessthan_approx(`a ignore, absRval_t r1, absRval_t r2);

static list_t<tag_cmp_t> join_tag_cmps(list_t<tag_cmp_t,`H> l1, 
				       list_t<tag_cmp_t,`H> l2) {
  if(l1==l2) return l1; // if not ptr-equal, just build a new list
  let res = NULL;
  for(; l2 != NULL; l2 = l2->tl) {
    let &TagCmp(cmp2,bd2) = l2->hd;
    // look through l1 for the best joined comparison we can find
    bool found = false;
    primop_t joined_cmp = Lte; // definite assignment too weak
    for(let old = l1; old != NULL; old = old->tl) {
      let &TagCmp(cmp1,bd1) = old->hd;
      // typecmp heavyweight? (could at least check ptrequal first)
      if(Tcutil::typecmp(bd2,bd1) == 0) {
	found = true;
	if(cmp1==cmp2) {
	  joined_cmp = cmp1;
	  break;
	}
      }
    }
    if(found)
      res = new List(new TagCmp(joined_cmp,bd2), res);
  }
  return res;
}

// note: There is another reasonable policy for losing an alias to AllInit:
//   (when destination unescaped!)
//   still make the result ThisInit and **don't** lose the aliases downstream.
//   I imagine the default should be AllInit and lose downstream with a pragma
//   in the source to get the other behavior (if anyone actually cares).
//   Note that the two policies are incomparable in what they accept.
// note: we don't have to record changes because anything changed would
//       have to have already been added as a change by r1 or r2!
static absRval_t join_absRval(joinenv_t env,`a a,absRval_t r1,absRval_t r2) {
  if(r1==r2) return r1;

  switch($(r1,r2)) { // only break when the answer should be Unknown or Esc!

  case $(&AddressOf(p1), &AddressOf(p2)):
    if(place_cmp(p1,p2)==0) return r1;
    add_place(env->pile,p1);
    add_place(env->pile,p2);
    break;
  case $(&AddressOf(p), _): add_place(env->pile,p); break;
  case $(_, &AddressOf(p)): add_place(env->pile,p); break;

  case $(NotZeroAll,  NotZeroThis):
  case $(NotZeroThis, NotZeroAll): return NotZeroThis;

  case $(&Aggregate(d1), &Aggregate(d2)):
    let new_d = Dict::union_two_c(join_absRval, env, d1, d2);
    if(new_d == d1) return r1;
    if(new_d == d2) return r2;
    return new Aggregate(new_d);
    
  case $(&TagCmps(l1),&TagCmps(l2)):
    let new_l = join_tag_cmps(l1,l2);
    if(new_l==l1) return r1;
    return new TagCmps(new_l);

    // KLUDGE: see above -- TagCmps are fully initialized
  case $(&TagCmps(_),_): return r2;
  case $(_,&TagCmps(_)): return r1;
    
  default: break;
  }
  initlevel_t il1 = initlevel(env->d1,r1);
  initlevel_t il2 = initlevel(env->d2,r2);
  switch($(r1,r2)) {
  case $(&Esc(_),_):
  case $(_,&Esc(_)):
    switch($(il1,il2)) {
    case $(_,NoneIL): case $(NoneIL,_): return esc_none;
    case $(_,ThisIL): case $(ThisIL,_): return esc_this;
    default: return esc_all;
    }
  default:
    switch($(il1,il2)) {
    case $(_,NoneIL): case $(NoneIL,_): return unknown_none;
    case $(_,ThisIL): case $(ThisIL,_): return unknown_this;
    default: return unknown_all;
    }
  }
}

static bool same_relop(reln_op_t r1,reln_op_t r2) {
  if (r1 == r2) return true;
  switch ($(r1,r2)) {
  case $(&EqualConst(c1),&EqualConst(c2)): return c1 == c2;
  case $(&LessVar(v1),   &LessVar(v2)):    return v1 == v2;
  case $(&LessSize(v1),  &LessSize(v2)):   return v1 == v2;
  case $(&LessConst(c1), &LessConst(c2)):  return c1 == c2;
  case $(&LessEqSize(v1),&LessEqSize(v2)): return v1 == v2;
  default: return false;
  }
}
static relns_t join_relns(relns_t r1s, relns_t r2s) {
  if (r1s == r2s) return r1s;
  //  fprintf(stderr,"join on ["); print_relns(r1s); 
  //  fprintf(stderr,"] and ["); print_relns(r2s); fprintf(stderr,"]\n");
  list_t<reln_t> res = NULL;
  bool diff = false;
  for (let r1s = r1s; r1s != NULL; r1s = r1s->tl) {
    let r1 = r1s->hd;
    bool found = false;
    for (let r2s = r2s; r2s != NULL; r2s = r2s->tl) {
      let r2 = r2s->hd;
      if (r1 == r2 || ((r1->vd == r2->vd) && same_relop(r1->rop,r2->rop))) {
        res = new List(r1,res);
        found = true;
        break;
      }
    }
    if (!found) diff = true;
  }
  if (!diff) res = r1s;
  //  fprintf(stderr,"result ["); print_relns(res); fprintf(stderr,"]\n");
  return res;
}
// for merging the "may consume" lists of two flows.
static void remove_f(List::list_t<place_t<`H,`H>> @l,
		     place_t<`H,`H> place) {
  try {
    *l = List::delete_cmp(place_cmp,*l,place);
    DEBUG_PRINT("  --deleted elem\n");
  } catch {
  case Core::Not_found:
    break; // no sweat if it's not there
  }
}
void print_place_list(List::list_t<place_t> x) {
  let first = true;
  while (x != NULL) {
    if (!first) { fprintf(stderr,", "); first = false; }
    print_place(x->hd);
    x = x->tl;
  }
  fprintf(stderr,"\n");
}

static consume_t or_consume(consume_t c1, consume_t c2) {
  DEBUG_PRINT("in or_consume:\n");
  DEBUG_PRINT("  c1.may_consume = ");
  DEBUG_PRINT_F(print_place_list,c1.may_consume);
  DEBUG_PRINT("  c2.may_consume = ");
  DEBUG_PRINT_F(print_place_list,c2.may_consume);
  // merge the may_consume lists
  List::list_t<place_t<`H,`H>,`H> mc = c2.may_consume;
  region r {
    // duplicate the first list, since it will be side-effected by delete.
    let x = rcopy(r,c1.may_consume);
    List::iter_c(remove_f,&x,c2.may_consume);
    // now concatenate the two lists
    while (x != NULL) {
      mc = new List::List(x->hd, mc);
      x = x->tl;
    }
  }
  // merge the consume sets: just union
  DEBUG_PRINT("  final may_consume = ");
  DEBUG_PRINT_F(print_place_list,mc);
  return ConsumeInfo(union_place_set(c1.consumed,c2.consumed,false),mc);
}

// the join above treats things like "or"; this is treating them
//   like "and"
consume_t and_consume(consume_t c1, consume_t c2) {
  // concatenate the may_consume lists
  let mc = List::append(c1.may_consume, c2.may_consume);
  // merge the consume sets; duplicates indicate an error
  return ConsumeInfo(union_place_set(c1.consumed,c2.consumed,true),mc);
}

flow_t join_flow(place_set_t* all_changed, flow_t f1, flow_t f2,
		 bool or_consumed){
  if(f1 == f2) return f1;
  switch($(f1,f2)) {
  case $(BottomFL,_): return f2;
  case $(_,BottomFL): return f1;
  case $(&ReachableFL(d1,r1,c1), &ReachableFL(d2,r2,c2)):
    // A lot of computation to avoid allocation, but profiling suggests
    // that's the right thing to do.
    if(d1 == d2 && r1 == r2 && 
       c1.may_consume == c2.may_consume &&
       c1.consumed == c2.consumed) return f1;
    if(flow_lessthan_approx(f1,f2)) return f2;
    if(flow_lessthan_approx(f2,f1)) return f1;
    region rgn {
      let env     = JoinEnv(rnew(rgn) EscPile(rgn,NULL),d1,d2);
      let outdict = Dict::intersect_c(join_absRval,&env,d1,d2);
      let r       = join_relns(r1,r2);
      let c       = or_consumed ? or_consume(c1,c2) : and_consume(c1,c2);
      return new ReachableFL(escape_these(env.pile, all_changed, outdict),r,c);
    }
  }
}

$(flow_t,absRval_t) join_flow_and_rval(place_set_t* all_changed,
				       $(flow_t,absRval_t) pr1,
				       $(flow_t,absRval_t) pr2,
				       bool or_consumed) {
  // possibly wasteful because it could do the escape shtuff twice
  // and it can because of && and || using 0 which abstracts to Zero. Sigh.
  let $(f1,r1) = pr1;
  let $(f2,r2) = pr2;
  flow_t outflow = join_flow(all_changed,f1,f2,or_consumed);
  switch($(f1,f2,outflow)) {
  case $(BottomFL,_,_): return $(outflow,r2);
  case $(_,BottomFL,_): return $(outflow,r1);
  case $(&ReachableFL(d1,_,_),&ReachableFL(d2,_,_), 
	 &ReachableFL(outd,relns,cinfo)):
    if(absRval_lessthan_approx(0,r1,r2)) return $(outflow,r2);
    if(absRval_lessthan_approx(0,r2,r1)) return $(outflow,r1);
    region rgn {
      let env  = JoinEnv(rnew(rgn) EscPile(rgn,NULL),d1,d2);
      let outr = join_absRval(&env,0,r1,r2);
      return $(new ReachableFL(escape_these(env.pile,all_changed,outd),
			       relns,cinfo), outr);
    }
  default: throw new Core::Impossible("join_flow_and_rval: BottomFL outflow");
  }
}

static absRval_t after_absRval_d(afterenv_t,field_name_t,absRval_t,absRval_t);
static absRval_t after_absRval(afterenv_t env, absRval_t r1, absRval_t r2) {
  bool changed1 = env->changed == One || Dict::member(env->chg1,env->curr_place);
  bool changed2 = env->changed == Two || Dict::member(env->chg2,env->curr_place);
  if(changed1 && changed2)
    return join_absRval(&env->joinenv, 0, r1, r2);
  // checking for "is a contained object changed in the other pinfo" is
  // quadratic because we'll do it "all the way down", but in practice
  // "all the way down" is small.
  if(changed1) {
    if(!prefix_of_member(env->joinenv.pile->rgn, env->curr_place, env->chg2))
      return r1;
    env->changed = One;
  }
  if(changed2) {
    if(!prefix_of_member(env->joinenv.pile->rgn, env->curr_place, env->chg1))
      return r2;
    env->changed = Two;
  }
  // neither one change, but they are not pointer equal.  Thus we assume
  // that they point to things that have changed.
  switch($(r1,r2)) {
  case $(&Aggregate(d1),&Aggregate(d2)):
    let new_d = Dict::union_two_c(after_absRval_d,env,d1,d2);
    if(new_d == d1) return r1;
    if(new_d == d2) return r2;
    return new Aggregate(new_d);
  default: throw new Core::Impossible("after_absRval -- non-aggregates!");
  }
}
static absRval_t after_absRval_d(afterenv_t env, field_name_t key,
				 absRval_t r1, absRval_t r2) {
  if(r1 == r2) return r1;
  let old_last_field = env->last_field_cell; 
  let old_changed    = env->changed;
  *env->last_field_cell = new List(key,NULL);
  env->last_field_cell  = &(*env->last_field_cell)->tl;
  let ans = after_absRval(env,r1,r2);
  env->last_field_cell        = old_last_field;
  (*env->last_field_cell)->tl = NULL;
  env->changed                = old_changed;
  return ans;
}
static absRval_t after_root(afterenv_t env, root_t root,
			    absRval_t r1, absRval_t r2) {
  if(r1 == r2) return r1;
  *env->curr_place     = Place(root,NULL);
  env->last_field_cell = &env->curr_place->fields;
  env->changed         = Neither;
  return after_absRval(env, r1, r2);
}
// FIX: region-allocate chg1 and chg2
flow_t after_flow(place_set_t* all_changed, flow_t f1, flow_t f2,
		  place_set_t chg1, place_set_t chg2) {
  static tunion Raw_exp.Const_e dummy_rawexp = Const_e(Null_c);
  static struct Exp dummy_exp = Exp(NULL,&dummy_rawexp,NULL,EmptyAnnot);
  static tunion Root.MallocPt dummy_root = MallocPt(&dummy_exp,VoidType);

  if(f1 == f2) return f1;
  switch($(f1,f2)) {
  case $(BottomFL,_): 
  case $(_,BottomFL): return BottomFL;
  case $(&ReachableFL(d1,r1,c1),&ReachableFL(d2,r2,c2)):
    if(d1 == d2 && r1 == r2 && 
       c1.may_consume == c2.may_consume &&
       c1.consumed == c2.consumed) return f1;
    // Should think about ways to avoid allocation here: (curr_place and
    // cut-off in presence of changes)
    region rgn {
      let curr_place = new Place(&dummy_root,NULL);
      let env  = AfterEnv(JoinEnv(rnew(rgn) EscPile(rgn,NULL), d1, d2), 
			  chg1, chg2, 
			  curr_place, &curr_place->fields,
			  Neither);
      let new_d = Dict::union_two_c(after_root,&env,d1,d2);
      let new_r = join_relns(r1,r2);
      let new_c = and_consume(c1,c2);
      return new ReachableFL(escape_these(env.joinenv.pile,all_changed,new_d),
                             new_r,new_c);
    }
  }
}

static bool tag_cmps_lessthan_approx(list_t<tag_cmp_t> l1, 
				     list_t<tag_cmp_t> l2) {
  // does a simple subset rather than a semantic implication.  Hey we're approx.
  for(; l2 != NULL; l2 = l2->tl) {
    let &TagCmp(cmp1,bd1) = l2->hd;
    let l = l1;
    for(; l != NULL; l=l->tl) {
      let &TagCmp(cmp2,bd2) = l->hd;
      // typecmp heavyweight? (could at least check ptrequal first)
      if(cmp2 == cmp1 && Tcutil::typecmp(bd2,bd1) == 0)
	break;
    }
    if(l==NULL)
      return false;
  }
  return true;
}

// WARNING: This is where we cheat ("approx") for performance.
static bool absRval_lessthan_approx(`a ignore, absRval_t r1, absRval_t r2) {
  if(r1 == r2) return true;

  switch ($(r1,r2)) {
  case $(&AddressOf(p1), &AddressOf(p2)): return place_cmp(p1,p2)==0;
  case $(&AddressOf(_),_):
  case $(_,&AddressOf(_)): return false;
  case $(&Aggregate(d1), &Aggregate(d2)):
    return (d1 == d2) || Dict::forall_intersect(absRval_lessthan_approx,d1,d2);
  case $(_, NotZeroThis): return r1==NotZeroAll;
  case $(_, Zero): 
  case $(_, NotZeroAll): return false;
  case $(&Esc(_),&Esc(_)): break;
  case $(&Esc(_),_): return false;
  case $(&TagCmps(l1),&TagCmps(l2)): return tag_cmps_lessthan_approx(l1,l2);
  case $(_,&TagCmps(_)): return false;
  default: break;
  }
  switch($(initlevel_approx(r1), initlevel_approx(r2))) {
  case $(AllIL,AllIL): return true;
  case $(_,NoneIL): return true;
  case $(NoneIL,_): return false;
  case $(_,ThisIL): return true;
  case $(ThisIL,_): return false;
  }
}

static bool relns_approx(relns_t r2s, relns_t r1s) {
  if (r1s == r2s) return true;
  //fprintf(stderr,"approx on ["); print_relns(r1s); 
  //fprintf(stderr,"] and ["); print_relns(r2s); fprintf(stderr,"]");
  for (; r1s != NULL; r1s = r1s->tl) {
    let r1 = r1s->hd;
    bool found = false;
    for (let r2s = r2s; r2s != NULL; r2s = r2s->tl) {
      let r2 = r2s->hd;
      if (r1 == r2 || ((r1->vd == r2->vd) && same_relop(r1->rop,r2->rop))) {
        found = true;
        break;
      }
    }
    if (!found)
      return false; 
  }
  return true;
}

bool consume_approx(consume_t c1, consume_t c2) {
  // first check that c2 is a sublist of c1; assumes they are sorted
  if (c1.may_consume != c2.may_consume) {
    let l2 = c2.may_consume;
    let l1 = c1.may_consume;
    for (; l1 != NULL && l2 != NULL; l1 = l1->tl) {
      if ((unsigned int)l1 == (unsigned int)l2)
	goto compare_consumed_set;
      let i = place_cmp(l1->hd,l2->hd);
      if (i == 0) // if equal, move l2's pointer to the next element
	l2 = l2->tl;
    }
    if (l2 != NULL) // didn't find them all
      return false;
  }
 compare_consumed_set:
  // now check that c1 is a superset of c2
  return place_set_subset(c2.consumed, c1.consumed);
}

// WARNING: We assume anything not in the intersection is IRRELEVANT.
// WARNING: We might return false even when f1 < f2, but that's okay b/c
//  that will just cause clients to take joins/afters and we always return true
//  when the two flows are equal.  That's what the approx means.
// (As a result of the approx, we don't have to allocate here.)
bool flow_lessthan_approx(flow_t f1, flow_t f2) {
  if(f1 == f2) return true;
  switch($(f1,f2)) {
  case $(BottomFL,_): return true;
  case $(_,BottomFL): return false;
  case $(&ReachableFL(d1,r1,c1),&ReachableFL(d2,r2,c2)):
    if(d1 == d2 && r1 == r2 &&
       c1.may_consume == c2.may_consume &&
       c1.consumed == c2.consumed) return true;
    return Dict::forall_intersect(absRval_lessthan_approx,d1,d2) &&
      relns_approx(r1,r2) && consume_approx(c1,c2);
  }
}

relns_t reln_kill_var(relns_t rs, vardecl_t v) {
  relns_t p;
  bool found = false;
  for (p = rs; !found && p != NULL; p = p->tl) {
    let r = p->hd;
    if (r->vd == v) { found = true; break; }
    switch (r->rop) {
    case &LessVar(v2):   fallthru(v2);
    case &LessSize(v2):  fallthru(v2);
    case &LessEqSize(v2): if(v==v2) found = true; break;
    default: break;
    }
  }
  if (!found) return rs;

  let new_rs = NULL;
  for (p = rs; p != NULL; p = p->tl) {
    let r = p->hd;
    if (r->vd != v) {
      switch (r->rop) {
      case &LessVar(v2):   fallthru(v2);
      case &LessSize(v2):  fallthru(v2);
      case &LessEqSize(v2): if (v == v2) continue; break;
      default: break;
      }
      new_rs = new List(r,new_rs);
    }
  }
  return new_rs;
}

relns_t reln_kill_exp(relns_t r, exp_t e) {
  switch (e->r) {
  case &Var_e(_,&Global_b(vd)): fallthru(vd);
  case &Var_e(_,&Param_b(vd)):  fallthru(vd);
  case &Var_e(_,&Local_b(vd)):  fallthru(vd);
  case &Var_e(_,&Pat_b(vd)):
    if (!vd->escapes)
      return reln_kill_var(r, vd);
    break;
  default: break;
  }
  return r;
}


relns_t reln_assign_var(relns_t r, vardecl_t v, exp_t e) {
  //fprintf(stderr,"assigning %s to %s: \n", Absynpp::qvar2string(v->name),
  //        Absynpp::exp2string(e));
  if (v->escapes) return r;
  // even if v isn't an integral type, it might be an array in which case
  // we need to kill it.
  r = reln_kill_var(r, v);
  // if e is malloc(n*sizeof(t)) or a calloc(n,sizeof(t)) 
  // then we can add n <= v.size.  Note that we can't easily
  // do this for comprehensions (i.e., rnew(e1) {for i < n : e2})
  // because e1 or e2 could modify n.  
  switch (e->r) {
  case &Malloc_e(MallocInfo{_,_,_,e2,true}):
    malloc_loop:
    switch (e2->r) {
    case &Cast_e(_,e3,_,_): e2 = e3; goto malloc_loop;
    case &Var_e(_,&Pat_b(n)): fallthru(n);
    case &Var_e(_,&Local_b(n)): fallthru(n);
    case &Var_e(_,&Param_b(n)): fallthru(n);
    case &Var_e(_,&Global_b(n)): 
      if (n->escapes) return r;
      return new List(new Reln{n,new LessEqSize(v)},r);
    default: return r;
    }
  default: break;
  }
  // otherwise ignore any situations where we have a non-integral type
  switch (Tcutil::compress(v->type)) {
  case &IntType(_,_): break;
  default: 
    return r;
  }
  // ignore if e isn't y.size or a constant
  
 loop:
  switch (e->r) {
  case &Cast_e(_,e1,_,_): e = e1; goto loop;
  case &Const_e(&Int_c(_,i)): 
    return new List(new Reln(v,new EqualConst(i)),r);
  case &Primop_e(Mod, &List(_,&List(e2,_))):
    // FIX: is this safe?  what if e modifies v2?
    // when we have x = e % v2.size, add x < v2.size
    switch (e2->r) {
    case &Primop_e(Size, &List(e3,_)):
      switch (e3->r) {
      case &Var_e(_,&Global_b(v2)): fallthru(v2);
      case &Var_e(_,&Local_b(v2)):  fallthru(v2);
      case &Var_e(_,&Param_b(v2)):  fallthru(v2);
      case &Var_e(_,&Pat_b(v2)):
        if (v2->escapes) return r;
        return new List(new Reln(v,new LessSize(v2)),r);
      default: break;
      }
      break;
    default: break;
    }
    break;
  case &Primop_e(Size,&List(e2,_)):
    switch (e2->r) {
    case &Var_e(_,&Global_b(v2)): fallthru(v2);
    case &Var_e(_,&Local_b(v2)):  fallthru(v2);
    case &Var_e(_,&Param_b(v2)):  fallthru(v2);
    case &Var_e(_,&Pat_b(v2)):
      if (v2->escapes) return r;
      return new List(new Reln(v,new LessEqSize(v2)),r);
    default: break;
    }
    break;
  default: break;
  }
  return r;
}

relns_t reln_assign_exp(relns_t r, exp_t e1, exp_t e2) {
  switch (e1->r) {
  case &Var_e(_,&Global_b(vd)): fallthru(vd);
  case &Var_e(_,&Param_b(vd)):  fallthru(vd);
  case &Var_e(_,&Local_b(vd)):  fallthru(vd);
  case &Var_e(_,&Pat_b(vd)):
    if (!vd->escapes)
      return reln_assign_var(r, vd, e2);
    break;
  default: break;
  }
  return r;
}

static void print_reln(reln_t r) {
  fprintf(stderr,"%s",qvar2string(r->vd->name));
  switch (r->rop) {
  case &EqualConst(c): fprintf(stderr,"==%d",c); break;
  case &LessVar(vd):   fprintf(stderr,"<%s",qvar2string(vd->name)); break;
  case &LessSize(vd):  fprintf(stderr,"<%s.size",qvar2string(vd->name)); break;
  case &LessConst(c):  fprintf(stderr,"<%d",c); break;
  case &LessEqSize(vd): fprintf(stderr,"<=%s.size",qvar2string(vd->name));break;
  }
}
void print_relns(relns_t r) {
  for (; r != NULL; r = r->tl) {
    print_reln(r->hd);
    if (r->tl != NULL) fprintf(stderr,",");
  }
}

void print_initlevel(initlevel_t il) {
  switch (il) {
  case NoneIL: fprintf(stderr,"uninitialized"); break;
  case ThisIL: fprintf(stderr,"this-initialized"); break;
  case AllIL: fprintf(stderr,"all-initialized"); break;
  }
}

void print_root(root_t root) {
  switch (root) {
  case &VarRoot(vd): 
    fprintf(stderr,"Root(%s): ",Absynpp::qvar2string(vd->name)); break;
  case &MallocPt(_,_):
    fprintf(stderr,"MallocPt(_,_)"); break;
  case &InitParam(_,_):
    fprintf(stderr,"InitParam(_,_)"); break;
  }
}

void print_place(place_t p) {
  print_root(p->root);
  if (p->fields != NULL) fprintf(stderr,"+(...)");
}

void print_place_set(place_set_t p) {
  region r {
    try {
      let iter = Dict::make_iter(r,p);
      let elem = *Dict::rchoose(r,p);
      bool first = true;
      fprintf(stderr,"{ ");
      while(Iter::next(iter,&elem)) {
	let p = elem[0];
	if (!first) {
	  fprintf(stderr,", ");
	  first = false;
	}
	print_place(p);
      }
      fprintf(stderr,"}\n");
    } catch {
    case Absent:
      fprintf(stderr,"{ }\n");
      return;
    }
  }
}

void print_absrval(absRval_t rval) {
  switch (rval) {
  case Zero: fprintf(stderr,"Zero"); break;
  case NotZeroAll: fprintf(stderr,"NotZeroAll"); break;
  case NotZeroThis: fprintf(stderr,"NotZeroThis"); break;
  case &UnknownR(il): fprintf(stderr,"Unknown("); print_initlevel(il);
    fprintf(stderr,")"); break;
  case &Esc(il): fprintf(stderr,"Esc("); print_initlevel(il);
    fprintf(stderr,")"); break;
  case &AddressOf(p): fprintf(stderr,"AddrOf("); print_place(p); 
    fprintf(stderr,")"); break;
  case &TagCmps(_): fprintf(stderr,"TagCmps(?)"); break;
  case &Aggregate(d): fprintf(stderr,"Aggregate(?)"); break;
  }
}

static void print_flow_mapping(root_t root, absRval_t rval) {
  fprintf(stderr,"    ");
  print_root(root);
  fprintf(stderr," --> ");
  print_absrval(rval);
  fprintf(stderr,"\n");
}

void print_flowdict(flowdict_t d) {
  Dict::iter(print_flow_mapping,d);
}

void print_flow(flow_t f) {
  switch (f) {
  case BottomFL: fprintf(stderr,"  BottomFL\n"); break;
  case &ReachableFL(fd,_,cinfo):
    fprintf(stderr,"  ReachableFL:\n");
    Dict::iter(print_flow_mapping,fd);
    fprintf(stderr,"\n    consumed: ");
    print_place_set(cinfo.consumed);
    fprintf(stderr,"\n    may_consume: ");
    print_place_list(cinfo.may_consume);
    fprintf(stderr,"\n");
    break;
  }
}

// returns true when t contains the region rgn with a few exceptions:
//  * doesn't count rgn in function types
//  * doesn't count rgn in region_t types (i.e., handles)
// We don't care about functions -- they're only callable if their 
// effects are covered by the capability and in the absence of closures,
// can't have any dangling pointers into rgn.  Region handles aren't
// "killed" when we reset a region and in fact we need them to ensure
// that we can allocate stuff in the region.  
bool contains_region(killrgn_t rgn, type_t t) {
  switch (Tcutil::compress(t)) {
  case VoidType: // these cases can't contain rgn
  case &IntType(_,_):    
  case FloatType:        
  case &DoubleType(_):   
  case &EnumType(_,_):   
  case &AnonEnumType(_): 
  case &SizeofType(_):   
  case &TagType(_):      
  case &TypeInt(_):      
  case HeapRgn:
  case &Evar(_,_,_,_): return false; // because of compress above
  case UniqueRgn:
    switch (rgn) {
    case UniqueRgn_k: return true;
    default: return false;
    }
  case &VarType(tv): 
    switch (rgn) {
    case UniqueRgn_k: return false;
    case &Region_k(rgn): return tvar_cmp(tv,rgn) == 0;
    }
    // for parameterized types, we just look to see if rgn shows up 
    // in the parameters -- this is overly aggressive since the regions
    // might not actually get used or may only get used in a handle or
    // in a function.  But it beats substituting and descending.
    //
    // Note also that we don't descend into the definitions of [x]tunions,
    // structs, typedefs, etc. because they must be closed w.r.t. all regions
    // except the heap.  
  case &TunionType(TunionInfo{_,targs,r}): 
    if (contains_region(rgn,r)) return true;
    fallthru(targs);
  case &TypedefType(_,targs,_,_):    fallthru(targs);
  case &AggrType(AggrInfo{_,targs}): fallthru(targs);
  case &TunionFieldType(TunionFieldInfo{_,targs}): fallthru(targs);
  case &JoinEff(targs): 
    return List::exists_c(contains_region,rgn,targs);
  case &PointerType(PtrInfo{t2,_,PtrAtts(r,_,_,_,_)}):
    return contains_region(rgn,r) || contains_region(rgn,t2);
  case &FnType(_): return false;
  case &TupleType(tqs):
    for (; tqs != NULL; tqs = tqs->tl)
      if (contains_region(rgn,(*tqs->hd)[1])) return true;
    return false;
  case &AnonAggrType(_,fds):
    for (; fds != NULL; fds = fds->tl) 
      if (contains_region(rgn,fds->hd->type)) return true;
    return false;
  case &ArrayType(ArrayInfo{t2,_,_,_,_}): fallthru(t2);
  case &AccessEff(t2):     fallthru(t2);
  case &RgnsEff(t2):       return contains_region(rgn,t2);
  // careful: we don't want to kill any region handles to the region in
  // question.
  case &RgnHandleType(t2): return false;
  }
}

// used by kill_flowdict_region below
static void kill_root($(flowdict_t,killrgn_t)@ env, root_t root, absRval_t rval) {
  let &$(*fd,rgn) = env;
  switch (root) {
  case &VarRoot(vd): 
    // if the variable's type contains the region
    if (contains_region(rgn,vd->type))
      // set it to uninitialized in the new dictionary
      rval = typ_to_absrval(vd->type, unknown_none);
    *fd = Dict::insert(*fd,root,rval);
    break;
  case &MallocPt(_,t): 
    // if the result of the malloc's type does not contain the region
    if (!contains_region(rgn,t))
      // insert the mapping into the new dictionary
      *fd = Dict::insert(*fd,root,rval);
    break;
  case &InitParam(_,_):
    // when we have interprocedural resettable, this could be a problem?!
    break;
  }
}

// "kill" all roots in the flowdict that have rgn in their type
static flowdict_t kill_flowdict_region(flowdict_t fd, type_t rgn) {
  switch (Tcutil::compress(rgn)) {
  case &VarType(tv): 
    tunion KillRgn.Region_k v = Region_k(tv);
    tunion _ KillRgn v2 = &v;
    $(flowdict_t,killrgn_t) env = $(Dict::empty(root_cmp),v2);
    Dict::iter_c(kill_root,&env,fd);
    return env[0];
    
  case UniqueRgn: 
    $(flowdict_t,killrgn_t) env = $(Dict::empty(root_cmp),UniqueRgn_k);
    Dict::iter_c(kill_root,&env,fd);
    return env[0];

  default: throw new Core::Impossible("kill_flowdict_region");
  }
}

// "kill" all relations in the flow analysis that involve the given region
static relns_t kill_relns_region(relns_t relns, type_t rgn) {
  // FIX: too conservative
  return NULL;
}

// the region rgn has been reset -- produce an outflow where the 
// roots involving rgn are now uninitialized.
flow_t kill_flow_region(flow_t f, type_t rgn) {
  switch (f) {
  case BottomFL: return f;
  case &ReachableFL(fd,r,c): 
    let fd2 = kill_flowdict_region(fd,rgn);
    let r2 = kill_relns_region(r,rgn);
    // FIX: deal with killing consumed values?
    return new ReachableFL(fd2,r2,c);
  }
}

static void consume_f($(place_set_t @, Position::seg_t) @env, 
		      place_t<`H,`H> place) {
  let &$(consume, loc) = env;
  DEBUG_PRINT("  adding ");
  DEBUG_PRINT_F(print_place,place);
  DEBUG_PRINT("  to consumed\n");
  if (update_place_set(consume, place, loc)) {
    let consume_loc = Dict::lookup(*consume,place);
    unique_err(place, "May consume unique pointer %s more than once (cf. %s)",
	       "May consume unique pointer %s more than once",
	       consume_loc, loc);
  }
}

// for each place in the may consume list, add it to the consume list;
//   if it's already there, then this is a conflict
flow_t consume_unique_rvals(Position::seg_t loc, flow_t f) {
  switch(f) {
  case BottomFL: return f;
  case &ReachableFL(fd,relns,cinfo):
    if (cinfo.may_consume == NULL) return f;
    let x = cinfo.consumed;
    List::iter_c(consume_f,new $(&x,loc),cinfo.may_consume);
    return new ReachableFL(fd,relns,ConsumeInfo(x,NULL));
  }
}

// Make sure none of the values in the may_consume list appear in the
// consumed set.
void check_unique_rvals(Position::seg_t loc, flow_t f) {
  switch (f) {
  case BottomFL: return;
  case &ReachableFL(d,r,c):
    let x = c.may_consume;
    // make sure these haven't been consumed
    while (x != NULL) {
      if (Dict::member(c.consumed,x->hd)) {
	let consume_loc = Dict::lookup(c.consumed, x->hd);
	unique_err(x->hd, 
		   "Read through possibly consumed unique pointer %s (cf. %s)",
		   "Read through possibly consumed unique pointer %s",
		   consume_loc, loc);
	break;
      }
      x = x->tl;
    }
    return;
  }
}

// drop unique rvals.  This is the case where they are in read position,
// but we don't do anything with them (i.e. x;)
flow_t drop_unique_rvals(Position::seg_t loc, flow_t f) {
  switch (f) {
  case BottomFL: return f;
  case &ReachableFL(d,r,c):
    c.may_consume = NULL;
    return new ReachableFL(d,r,c);
  }
}

// When reading through a variable, simply make sure that the unique
// pointers are OK, and then purge the may_consume list
flow_t readthrough_unique_rvals(Position::seg_t loc, flow_t f) {
  check_unique_rvals(loc,f);
  switch (f) {
  case BottomFL: return f;
  case &ReachableFL(d,r,c):
    c.may_consume = NULL;
    return new ReachableFL(d,r,c);
  }
}

$(consume_t,flow_t) save_consume_info(flow_t f, bool clear) {
  static place_set_t *empty_info = NULL;
  switch (f) {
  case BottomFL: 
    if (empty_info == NULL)
      empty_info = new mt_place_set();
    return $(ConsumeInfo(*empty_info,NULL),f);
  case &ReachableFL(d,r,c):
    if (clear)
      return $(c,new ReachableFL(d,r,ConsumeInfo(mt_place_set(),NULL)));
    else
      return $(c,f);
  }
}

flow_t restore_consume_info(flow_t f, consume_t c) {
  switch (f) {
  case BottomFL: return f;
  case &ReachableFL(d,r,_):
    return new ReachableFL(d,r,c);
  }
}
