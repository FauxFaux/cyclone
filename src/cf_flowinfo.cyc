/* Utilities for control flow analysis.
   Copyright (C) 2001 Dan Grossman, Greg Morrisett
   This file is part of the Cyclone compiler.

   The Cyclone compiler is free software; you can redistribute it
   and/or modify it under the terms of the GNU General Public License
   as published by the Free Software Foundation; either version 2 of
   the License, or (at your option) any later version.

   The Cyclone compiler is distributed in the hope that it will be
   useful, but WITHOUT ANY WARRANTY; without even the implied warranty
   of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with the Cyclone compiler; see the file COPYING. If not,
   write to the Free Software Foundation, Inc., 59 Temple Place -
   Suite 330, Boston, MA 02111-1307, USA. */

#include <core.h>
#include <list.h>
#include <set.h>
#include <dict.h>
#include <string.h>
#include "absyn.h"
#include "tcutil.h"
#include "absynpp.h"

#define CF_FLOWINFO_CYC
#include "cf_flowinfo.h"

using Absyn;
using List;
using Absynpp;
namespace CfFlowInfo;

////////////////////////// Shared Constants //////////////////////////
static tunion AbsRVal.UnknownR unknown_none_v = UnknownR(NoneIL);
static tunion AbsRVal.UnknownR unknown_this_v = UnknownR(ThisIL);
static tunion AbsRVal.UnknownR unknown_all_v  = UnknownR(AllIL);
static tunion AbsRVal.Esc      esc_none_v     = Esc(NoneIL);
static tunion AbsRVal.Esc      esc_this_v     = Esc(ThisIL);
static tunion AbsRVal.Esc      esc_all_v      = Esc(AllIL);
absRval_t unknown_none = &unknown_none_v;
absRval_t unknown_this = &unknown_this_v;
absRval_t unknown_all  = &unknown_all_v;
absRval_t esc_none     = &esc_none_v;
absRval_t esc_this     = &esc_this_v;
absRval_t esc_all      = &esc_all_v;

static place_set_t * mt_place_set_opt = NULL;
place_set_t mt_place_set() {
  if(mt_place_set_opt == NULL)
    mt_place_set_opt = new Set::empty(place_cmp);
  return *mt_place_set_opt;
}

//////////////////////////// Utilities ///////////////////////////////
int root_cmp(root_t r1, root_t r2) {
  if(((int)r1) == ((int)r2))
    return 0;
  switch ($(r1,r2)) {
  case $(&VarRoot(vd1),&VarRoot(vd2)): return ((int) vd1) - ((int) vd2);
  case $(&VarRoot(_),_): return -1;
  case $(_,&VarRoot(_)): return 1;
  case $(&MallocPt(e1,_),&MallocPt(e2,_)): return ((int) e1) - ((int) e2);
  case $(&MallocPt(_,_),_): return -1;
  case $(_,&MallocPt(_,_)): return 1;
  case $(&InitParam(i1,_), &InitParam(i2,_)): return i1-i2;
  }
}
int place_cmp(place_t p1, place_t p2) {
  if(((int)p1) == ((int)p2))
    return 0;
  int i = root_cmp(p1->root,p2->root);
  if(i != 0)
    return i;
  return list_cmp(strptrcmp,p1->fields,p2->fields);
}

// for debugging
static mstringptr_t place2string(place_t p) {
  list_t<mstringptr_t> sl = NULL;
  switch(p->root) {
  case &VarRoot(vd): sl = new List(new aprintf("%s",*(*vd->name)[1]),sl); break;
  case &MallocPt(e,_):  sl = new List(new aprintf("mpt%d",(int)e),sl); break;
  case &InitParam(i,_): sl = new List(new aprintf("param%d",i),sl); break;
  }
  for(_ fields = p->fields; fields != NULL; fields = fields->tl)
    sl = new List(new aprintf("%s", *fields->hd), sl);
  let ans = new aprintf("%s","");
  for(; sl != NULL; sl = sl->tl)
    *ans = aprintf("%s.%s", *sl->hd, *ans);
  return ans;
}

// Note: We don't have to worry about instantiating polymorphic struct fields
//       b/c they can't be instantiated with aggregates.
// BUT that means fields instantiated with int still must be initialized.
// Sorry, but it's a pain.
// Also, not sure why enums aren't bits_only.
//   (If we ever have memory kinds, then we either need to figure out field's
//    instantiated types or not allow partial initialization of such fields.)
// Note: We could memoize answers here.
absRval_t i_typ_to_absrval(bool allow_zeroterm,
                           type_t t, absRval_t leafval) {
  if(!is_union_type(t))
    switch(Tcutil::compress(t)) {
    case &TunionFieldType(TunionFieldInfo(&KnownTunionfield(_,tuf),_)):
      if(tuf->typs == NULL)
	return leafval;
      fallthru(tuf->typs);
    case &TupleType(tqts):
      aggrdict_t d = Dict::empty(strptrcmp);
      for(int i=0; tqts != NULL; tqts = tqts->tl, ++i)
	d = Dict::insert(d,fieldname(i),
                         i_typ_to_absrval(false,(*tqts->hd)[1],leafval));
      return new Aggregate(d);

    case &AggrType(AggrInfo(info,_)):
      let ad = get_known_aggrdecl(info);
      if(ad->impl==NULL)
	break;
      fallthru(ad->impl->fields);
    case &AnonAggrType(StructA,fs):
      aggrdict_t d = Dict::empty(strptrcmp);
      for(; fs != NULL; fs = fs->tl) {
	let &Aggrfield(n,_,t2,_,_) = fs->hd;
	if((*n).size != 1) //empty name means bit field so no need to include it
	  d = Dict::insert(d, n, i_typ_to_absrval(false, t2,leafval));
      }
      return new Aggregate(d);
    case &ArrayType(ArrayInfo{et,_,_,zeroterm}) && conref_def(false,zeroterm):
      // special case for zero-terminated arrays of bits-only things -- 
      // here, we initialize them at translation by putting a 0 in at
      // the end.
      return (allow_zeroterm && Tcutil::bits_only(et)) ? unknown_all : leafval;
    default: break;
    }
  return Tcutil::bits_only(t) ? unknown_all : leafval;
}

absRval_t typ_to_absrval(type_t t, absRval_t leafval) {
  return i_typ_to_absrval(true, t, leafval);
}

static bool prefix_of_member(region_t<`r> r, place_t<`r2,`r3> place,
			     Set::set_t<place_t<`r2,`r3>> set) {
  let place2 = place;
  let iter = Set::make_iter(r,set);
  while(Iter::next(iter,&place2)) {
    if(root_cmp(place->root, place2->root) != 0)
      continue;
    let fs1 = place->fields;
    let fs2 = place2->fields;
    for(; fs1 != NULL && fs2 != NULL; fs1 = fs1->tl, fs2 = fs2->tl)
      if(strptrcmp(fs1->hd,fs2->hd) != 0)
	break;
    if(fs1 == NULL)
      return true;
  }
  return false;
}

//////////////////////// Managing Escapes ///////////////////////////

// Note: This would be a great place to delay an idempotent genregion
//       because we often won't have any places! (Also would want to pick
//       a very small initial region size.)
static struct EscPile<`r::R> {
  region_t<`r>       rgn;
  list_t<place_t,`r> places;
};
typedef struct EscPile<`r1> @`r2 escpile_t<`r1,`r2>;

static void add_place(escpile_t pile, place_t<`H,`H> place) {
  // we expect lots of inserts and short lists, so we don't bother to sort
  // we check for repeats to avoid a very unlikely exponential blowup
  if(!List::mem(place_cmp, pile->places, place))
    pile->places = rnew(pile->rgn) List(place, pile->places);
}
static void add_places(escpile_t pile, `a a, absRval_t r) {
  switch(r) {
  case &AddressOf(p): add_place(pile, p); return;
  case &Aggregate(d): Dict::iter_c(add_places, pile, d); return;
  default: return;
  }
}

// WARNING: do not use this to clobber with an Esc unless that's the point!
static absRval_t insert_place_inner(absRval_t new_val,absRval_t old_val) {
  switch(old_val) {
  case &Aggregate(d): return new Aggregate(Dict::map_c(insert_place_inner,
						       new_val, d));
  default: return new_val;
  }
}
static absRval_t insert_place_outer(list_t<field_name_t> fs,
				    absRval_t old_val, absRval_t new_val) {
  if(fs==NULL) 
    return insert_place_inner(new_val,old_val);
  switch($(fs,old_val)) {
  case $(&List(hd,tl),&Aggregate(d)):
    let new_child = insert_place_outer(tl,Dict::lookup(d,hd), new_val);
    return new Aggregate(Dict::insert(d, hd, new_child));
  default: throw new Core::Impossible("bad insert place");
  }
}
// Note: this terminates because we thread the dictionary.
// Note: we need to catch not in the dict b/c of how join_flow uses this code?
// Note: we should only be escaping leaves now -- never Aggregate!
// Note: There's a quadratic behavior here that won't be a problem in practice.
static flowdict_t escape_these(escpile_t pile, place_set_t * all_changed,
			       flowdict_t d) {
  while(pile->places != NULL) {
    let place = pile->places->hd;
    pile->places = pile->places->tl;
    if(all_changed != NULL)
      *all_changed = Set::insert(*all_changed, place);
    absRval_t oldval, newval;
    try oldval = lookup_place(d,place); 
    catch { case Dict::Absent: continue; }
    switch(initlevel(d,oldval)) {
    case AllIL:  newval = esc_all;  break;
    case ThisIL: newval = esc_this; break;
    case NoneIL: newval = esc_none; break;
    }
    add_places(pile, 0, oldval);
    d = Dict::insert(d,place->root,
		     insert_place_outer(place->fields,
					Dict::lookup(d,place->root),
					newval));
  }
  return d;
}

////////////////////////// Flow Gets and (functional) Sets ///////////////////

// We must detect cycles in the MustPointTo graph or we won't terminate.
// We heap-allocate the seen list b/c we expect it to be very very short.
static struct InitlevelEnv {
  flowdict_t d;
  list_t<place_t> seen;
};
static char dummy_str_v[1]ZEROTERM = "";
static const char ? dummy_str = dummy_str_v;
static initlevel_t initlevel_approx(absRval_t r) {
  switch(r) {
  case &UnknownR(il): return il;
  case &Esc(il):      return il;
  case Zero:
  case NotZeroAll:    return AllIL;
  case NotZeroThis:   return ThisIL;
  default: throw new Core::Impossible("initlevel_approx");
  }
}
static initlevel_t initlevel_rec(struct InitlevelEnv @ env, `a a, 
				 absRval_t r, initlevel_t acc) {
  initlevel_t this_ans;
  if(acc == NoneIL) return NoneIL;
  switch(r) {
  case &Aggregate(d): this_ans = Dict::fold_c(initlevel_rec,env,d,AllIL); break;
  case &AddressOf(p): 
    if(List::mem(place_cmp, env->seen, p))
      this_ans = AllIL;
    else {
      env->seen = new List(p, env->seen);
      this_ans  = initlevel_rec(env,0,lookup_place(env->d,p),AllIL);
      env->seen = env->seen->tl;
      if(this_ans == NoneIL)
	this_ans = ThisIL;
    }
    break;
  default: this_ans = initlevel_approx(r);
  }
  if(this_ans == NoneIL) return NoneIL;
  if(this_ans == ThisIL || acc == ThisIL) return ThisIL;
  return AllIL;
}
initlevel_t initlevel(flowdict_t d, absRval_t r) {
  let env = InitlevelEnv(d,NULL);
  return initlevel_rec(&env,0,r,AllIL);
}

absRval_t lookup_place(flowdict_t d, place_t place) {
  let &Place(root,fields) = place;
  let ans = Dict::lookup(d,root);
  for(; fields != NULL; fields = fields->tl)
    switch($(ans,fields->hd)) {
    case $(&Aggregate(d2), fname): 
      ans = Dict::lookup(d2,fname); 
      break;
    default: throw new Core::Impossible("bad lookup_place");
    }
  return ans;
}

static bool is_rval_unescaped(`a a, `b b, absRval_t rval) {
  switch(rval) {
  case &Esc(_):       return false;
  case &Aggregate(d): return Dict::forall_c(is_rval_unescaped,0,d);
  default:            return true;
  }
}
bool is_unescaped(flowdict_t d, place_t place) {
  return is_rval_unescaped(0,0,lookup_place(d,place));
}

// only things pointed to escape
flowdict_t escape_deref(flowdict_t d, place_set_t * all_changed, absRval_t r) {
  region rgn {
    escpile_t pile = rnew(rgn) EscPile(rgn,NULL);
    add_places(pile, 0, r);
    return escape_these(pile, all_changed, d);
  }
}

static struct AssignEnv<`r::R> {
  escpile_t<`r,`r> pile;
  flowdict_t       d;
  Position::seg_t  loc;
};
static absRval_t assign_place_inner(struct AssignEnv<`r> @ env, `a a,
				    absRval_t oldval, absRval_t newval) {
  // assignments to escaped places must be fully-init because other aliases
  // might assume fully-init.  Always causes place to be added for same reason.
  switch($(oldval,newval)) {
  case $(&Esc(_), &AddressOf(p)): add_place(env->pile,p); fallthru;
  case $(&Esc(_), _):
    if(initlevel(env->d,newval) != AllIL)
      Tcutil::terr(env->loc, "assignment puts possibly-uninitialized data in "
		   "an escaped location");
    return esc_all;
  case $(&Aggregate(d1), &Aggregate(d2)):
    // re-ordering dicts may reduce allocation?
    let new_d = Dict::union_two_c(assign_place_inner, env, d1, d2);
    if(new_d == d1) return oldval;
    if(new_d == d2) return newval;
    return new Aggregate(new_d);
  case $(_, &Esc(il)): // we already know we don't have an escaped location
    switch(il) {
    case NoneIL: return unknown_none;
    case ThisIL: return unknown_this;
    case AllIL:  return unknown_all;
    }
  default: return newval;
  }
}
static absRval_t assign_place_outer(struct AssignEnv<`r> @ env, 
				    list_t<field_name_t> fs,
				    absRval_t oldval, absRval_t newval) {
  if(fs == NULL) return assign_place_inner(env,0,oldval,newval);
  switch($(fs,oldval)) {
  case $(&List(hd,tl),&Aggregate(d)):
    let new_child = assign_place_outer(env,tl,Dict::lookup(d,hd),newval);
    return new Aggregate(Dict::insert(d, hd, new_child));
  default: throw new Core::Impossible("bad insert place");
  }
}
flowdict_t assign_place(Position::seg_t loc, flowdict_t d,
			place_set_t * all_changed, place_t<`H,`H> place, 
			absRval_t r) {
  if(all_changed != NULL)
    *all_changed = Set::insert(*all_changed,place);
  region rgn {
    let &Place(root,fs) = place;
    struct AssignEnv<`rgn> env = AssignEnv(rnew(rgn) EscPile(rgn,NULL), d, loc);
    absRval_t newval= assign_place_outer(&env,fs,Dict::lookup(d,root),r);
    return escape_these(env.pile, all_changed, Dict::insert(d, root, newval));
  }
}

/////////////////// Join, After, and Lessthan ///////////////////////////////
static struct JoinEnv<`r::R> {
  escpile_t<`r,`r> pile;
  flowdict_t       d1;
  flowdict_t       d2;
};
typedef struct JoinEnv<`r1> @`r2 joinenv_t<`r1,`r2>;
static enum WhoIsChanged { Neither, One, Two }; // if both changed, take join
static struct AfterEnv<`r::R> {
  struct JoinEnv<`r> joinenv;
  Set::set_t<place_t> chg1; // should region-allocate!
  Set::set_t<place_t> chg2; // should region-allocate!
  place_t curr_place; // a pain to stack-allocate, but should!
  list_t<field_name_t> @ last_field_cell;
  enum WhoIsChanged changed;
};
typedef struct AfterEnv<`r1> @`r2 afterenv_t<`r1,`r2>;

// note: There is another reasonable policy for losing an alias to AllInit:
//   (when destination unescaped!)
//   still make the result ThisInit and **don't** lose the aliases downstream.
//   I imagine the default should be AllInit and lose downstream with a pragma
//   in the source to get the other behavior (if anyone actually cares).
//   Note that the two policies are incomparable in what they accept.
// note: we don't have to record changes because anything changed would
//       have to have already been added as a change by r1 or r2!
static absRval_t join_absRval(joinenv_t env,`a a,absRval_t r1,absRval_t r2) {
  if(r1==r2) return r1;

  switch($(r1,r2)) { // only break when the answer should be Unknown or Esc!

  case $(&AddressOf(p1), &AddressOf(p2)):
    if(place_cmp(p1,p2)==0) return r1;
    add_place(env->pile,p1);
    add_place(env->pile,p2);
    break;
  case $(&AddressOf(p), _): add_place(env->pile,p); break;
  case $(_, &AddressOf(p)): add_place(env->pile,p); break;

  case $(NotZeroAll,  NotZeroThis):
  case $(NotZeroThis, NotZeroAll): return NotZeroThis;

  case $(&Aggregate(d1), &Aggregate(d2)):
    let new_d = Dict::union_two_c(join_absRval, env, d1, d2);
    if(new_d == d1) return r1;
    if(new_d == d2) return r2;
    return new Aggregate(new_d);
    
  default: break;
  }
  initlevel_t il1 = initlevel(env->d1,r1);
  initlevel_t il2 = initlevel(env->d2,r2);
  switch($(r1,r2)) {
  case $(&Esc(_),_):
  case $(_,&Esc(_)):
    switch($(il1,il2)) {
    case $(_,NoneIL): case $(NoneIL,_): return esc_none;
    case $(_,ThisIL): case $(ThisIL,_): return esc_this;
    default: return esc_all;
    }
  default:
    switch($(il1,il2)) {
    case $(_,NoneIL): case $(NoneIL,_): return unknown_none;
    case $(_,ThisIL): case $(ThisIL,_): return unknown_this;
    default: return unknown_all;
    }
  }
}
bool same_relop(reln_op_t r1,reln_op_t r2) {
  if (r1 == r2) return true;
  switch ($(r1,r2)) {
  case $(&EqualConst(c1),&EqualConst(c2)): return c1 == c2;
  case $(&LessVar(v1),   &LessVar(v2)):    return v1 == v2;
  case $(&LessSize(v1),  &LessSize(v2)):   return v1 == v2;
  case $(&LessConst(c1), &LessConst(c2)):  return c1 == c2;
  case $(&LessEqSize(v1),&LessEqSize(v2)): return v1 == v2;
  default: return false;
  }
}
relns_t join_relns(relns_t r1s, relns_t r2s) {
  if (r1s == r2s) return r1s;
  //  fprintf(stderr,"join on ["); print_relns(r1s); 
  //  fprintf(stderr,"] and ["); print_relns(r2s); fprintf(stderr,"]\n");
  list_t<reln_t> res = NULL;
  bool diff = false;
  for (let r1s = r1s; r1s != NULL; r1s = r1s->tl) {
    let r1 = r1s->hd;
    bool found = false;
    for (let r2s = r2s; r2s != NULL; r2s = r2s->tl) {
      let r2 = r2s->hd;
      if (r1 == r2 || ((r1->vd == r2->vd) && same_relop(r1->rop,r2->rop))) {
        res = new List(r1,res);
        found = true;
        break;
      }
    }
    if (!found) diff = true;
  }
  if (!diff) res = r1s;
  //  fprintf(stderr,"result ["); print_relns(res); fprintf(stderr,"]\n");
  return res;
}

flow_t join_flow(place_set_t* all_changed, flow_t f1, flow_t f2){
  if(f1 == f2) return f1;
  switch($(f1,f2)) {
  case $(BottomFL,_): return f2;
  case $(_,BottomFL): return f1;
  case $(&ReachableFL(d1,r1),&ReachableFL(d2,r2)):
    // A lot of computation to avoid allocation, but profiling suggests
    // that's the right thing to do.
    if(d1 == d2 && r1 == r2) return f1;
    if(flow_lessthan_approx(f1,f2)) return f2;
    if(flow_lessthan_approx(f2,f1)) return f1;
    region rgn {
      let env     = JoinEnv(rnew(rgn) EscPile(rgn,NULL),d1,d2);
      let outdict = Dict::intersect_c(join_absRval,&env,d1,d2);
      let r       = join_relns(r1,r2);
      return new ReachableFL(escape_these(env.pile, all_changed, outdict),r);
    }
  }
}

static absRval_t after_absRval_d(afterenv_t,field_name_t,absRval_t,absRval_t);
static absRval_t after_absRval(afterenv_t env, absRval_t r1, absRval_t r2) {
  bool changed1 = env->changed == One || Set::member(env->chg1,env->curr_place);
  bool changed2 = env->changed == Two || Set::member(env->chg2,env->curr_place);
  if(changed1 && changed2)
    return join_absRval(&env->joinenv, 0, r1, r2);
  // checking for "is a contained object changed in the other pinfo" is
  // quadratic because we'll do it "all the way down", but in practice
  // "all the way down" is small.
  if(changed1) {
    if(!prefix_of_member(env->joinenv.pile->rgn, env->curr_place, env->chg2))
      return r1;
    env->changed = One;
  }
  if(changed2) {
    if(!prefix_of_member(env->joinenv.pile->rgn, env->curr_place, env->chg1))
      return r2;
    env->changed = Two;
  }
  switch($(r1,r2)) {
  case $(&Aggregate(d1),&Aggregate(d2)):
    let new_d = Dict::union_two_c(after_absRval_d,env,d1,d2);
    if(new_d == d1) return r1;
    if(new_d == d2) return r2;
    return new Aggregate(new_d);
  default: throw new Core::Impossible("after_pathinfo -- non-aggregates!");
  }
}
static absRval_t after_absRval_d(afterenv_t env, field_name_t key,
				 absRval_t r1, absRval_t r2) {
  if(r1 == r2) return r1;
  let old_last_field = env->last_field_cell; 
  let old_changed    = env->changed;
  *env->last_field_cell = new List(key,NULL);
  env->last_field_cell  = &(*env->last_field_cell)->tl;
  let ans = after_absRval(env,r1,r2);
  env->last_field_cell        = old_last_field;
  (*env->last_field_cell)->tl = NULL;
  env->changed                = old_changed;
  return ans;
}
static absRval_t after_root(afterenv_t env, root_t root,
			    absRval_t r1, absRval_t r2) {
  if(r1 == r2) return r1;
  *env->curr_place     = Place(root,NULL);
  env->last_field_cell = &env->curr_place->fields;
  env->changed         = Neither;
  return after_absRval(env, r1, r2);
}
// FIX: region-allocate chg1 and chg2
flow_t after_flow(place_set_t* all_changed, flow_t f1, flow_t f2,
		  place_set_t chg1, place_set_t chg2) {
  static tunion Raw_exp.Const_e dummy_rawexp = Const_e(Null_c);
  static struct Exp dummy_exp = Exp(NULL,&dummy_rawexp,NULL,EmptyAnnot);
  static tunion Root.MallocPt dummy_root = MallocPt(&dummy_exp,VoidType);

  if(f1 == f2) return f1;
  switch($(f1,f2)) {
  case $(BottomFL,_): 
  case $(_,BottomFL): return BottomFL;
  case $(&ReachableFL(d1,r1),&ReachableFL(d2,r2)):
    if(d1 == d2 && r1 == r2) return f1;
    // Should think about ways to avoid allocation here: (curr_place and
    // cut-off in presence of changes)
    region rgn {
      let curr_place = new Place(&dummy_root,NULL);
      let env  = AfterEnv(JoinEnv(rnew(rgn) EscPile(rgn,NULL), d1, d2), 
			  chg1, chg2, 
			  curr_place, &curr_place->fields,
			  Neither);
      let new_d = Dict::union_two_c(after_root,&env,d1,d2);
      let new_r = join_relns(r1,r2);
      return new ReachableFL(escape_these(env.joinenv.pile,all_changed,new_d),
                             new_r);
    }
  }
}

// WARNING: This is where we cheat ("approx") for performance.
static bool absRval_lessthan_approx(`a ignore, absRval_t r1, absRval_t r2) {
  if(r1 == r2) return true;

  switch ($(r1,r2)) {
  case $(&AddressOf(p1), &AddressOf(p2)): return place_cmp(p1,p2)==0;
  case $(&AddressOf(_),_):
  case $(_,&AddressOf(_)): return false;
  case $(&Aggregate(d1), &Aggregate(d2)):
    return (d1 == d2) || Dict::forall_intersect(absRval_lessthan_approx,d1,d2);
  case $(_, NotZeroThis): return r1==NotZeroAll;
  case $(_, Zero): 
  case $(_, NotZeroAll): return false;
  case $(&Esc(_),&Esc(_)): break;
  case $(&Esc(_),_): return false;
  default: break;
  }
  switch($(initlevel_approx(r1), initlevel_approx(r2))) {
  case $(AllIL,AllIL): return true;
  case $(_,NoneIL): return true;
  case $(NoneIL,_): return false;
  case $(_,ThisIL): return true;
  case $(ThisIL,_): return false;
  }
}

bool relns_approx(relns_t r2s, relns_t r1s) {
  if (r1s == r2s) return true;
  //fprintf(stderr,"approx on ["); print_relns(r1s); 
  //fprintf(stderr,"] and ["); print_relns(r2s); fprintf(stderr,"]");
  for (; r1s != NULL; r1s = r1s->tl) {
    let r1 = r1s->hd;
    bool found = false;
    for (let r2s = r2s; r2s != NULL; r2s = r2s->tl) {
      let r2 = r2s->hd;
      if (r1 == r2 || ((r1->vd == r2->vd) && same_relop(r1->rop,r2->rop))) {
        found = true;
        break;
      }
    }
    if (!found) {
      //fprintf(stderr,"--false\n"); 
      return false; 
    }
  }
  //fprintf(stderr,"--true\n");
  return true;
}

// WARNING: We assume anything not in the intersection is IRRELEVANT.
// WARNING: We might return false even when f1 < f2, but that's okay b/c
//  that will just cause clients to take joins/afters and we always return true
//  when the two flows are equal.  That's what the approx means.
// (As a result of the approx, we don't have to allocate here.)
bool flow_lessthan_approx(flow_t f1, flow_t f2) {
  if(f1 == f2) return true;
  switch($(f1,f2)) {
  case $(BottomFL,_): return true;
  case $(_,BottomFL): return false;
  case $(&ReachableFL(d1,r1),&ReachableFL(d2,r2)):
    if(d1 == d2 && r1 == r2) return true;
    return Dict::forall_intersect(absRval_lessthan_approx,d1,d2) &&
      relns_approx(r1,r2);
  }
}

relns_t reln_kill_var(relns_t rs, vardecl_t v) {
  relns_t p;
  bool found = false;
  for (p = rs; !found && p != NULL; p = p->tl) {
    let r = p->hd;
    if (r->vd == v) { found = true; break; }
    switch (r->rop) {
    case &LessVar(v2):   fallthru(v2);
    case &LessSize(v2):  fallthru(v2);
    case &LessEqSize(v2): if(v==v2) found = true; break;
    default: break;
    }
  }
  if (!found) return rs;

  let new_rs = NULL;
  for (p = rs; p != NULL; p = p->tl) {
    let r = p->hd;
    if (r->vd != v) {
      switch (r->rop) {
      case &LessVar(v2):   fallthru(v2);
      case &LessSize(v2):  fallthru(v2);
      case &LessEqSize(v2): if (v == v2) continue; break;
      default: break;
      }
      new_rs = new List(r,new_rs);
    }
  }
  return new_rs;
}

relns_t reln_kill_exp(relns_t r, exp_t e) {
  switch (e->r) {
  case &Var_e(_,&Global_b(vd)): fallthru(vd);
  case &Var_e(_,&Param_b(vd)):  fallthru(vd);
  case &Var_e(_,&Local_b(vd)):  fallthru(vd);
  case &Var_e(_,&Pat_b(vd)):
    if (!vd->escapes)
      return reln_kill_var(r, vd);
    break;
  default: break;
  }
  return r;
}


relns_t reln_assign_var(relns_t r, vardecl_t v, exp_t e) {
  //fprintf(stderr,"assigning %s to %s: \n", Absynpp::qvar2string(v->name),
  //        Absynpp::exp2string(e));
  if (v->escapes) return r;
  // even if v isn't an integral type, it might be an array in which case
  // we need to kill it.
  r = reln_kill_var(r, v);
  // if e is malloc(n*sizeof(t)) or a calloc(n,sizeof(t)) 
  // then we can add n <= v.size.  Note that we can't easily
  // do this for comprehensions (i.e., rnew(e1) {for i < n : e2})
  // because e1 or e2 could modify n.  
  switch (e->r) {
  case &Malloc_e(MallocInfo{_,_,_,e2,true}):
    malloc_loop:
    switch (e2->r) {
    case &Cast_e(_,e3): e2 = e3; goto malloc_loop;
    case &Var_e(_,&Pat_b(n)): fallthru(n);
    case &Var_e(_,&Local_b(n)): fallthru(n);
    case &Var_e(_,&Param_b(n)): fallthru(n);
    case &Var_e(_,&Global_b(n)): 
      if (n->escapes) return r;
      return new List(new Reln{n,new LessEqSize(v)},r);
    default: return r;
    }
  default: break;
  }
  // otherwise ignore any situations where we have a non-integral type
  switch (Tcutil::compress(v->type)) {
  case &IntType(_,_): break;
  default: 
    return r;
  }
  // ignore if e isn't y.size or a constant
  
 loop:
  switch (e->r) {
  case &Cast_e(_,e1): e = e1; goto loop;
  case &Const_e(&Int_c(_,i)): 
    return new List(new Reln(v,new EqualConst(i)),r);
  case &Primop_e(Mod, &List(_,&List(e2,_))):
    // FIX: is this safe?  what if e modifies v2?
    // when we have x = e % v2.size, add x < v2.size
    switch (e2->r) {
    case &Primop_e(Size, &List(e3,_)):
      switch (e3->r) {
      case &Var_e(_,&Global_b(v2)): fallthru(v2);
      case &Var_e(_,&Local_b(v2)):  fallthru(v2);
      case &Var_e(_,&Param_b(v2)):  fallthru(v2);
      case &Var_e(_,&Pat_b(v2)):
        if (v2->escapes) return r;
        return new List(new Reln(v,new LessSize(v2)),r);
      default: break;
      }
      break;
    default: break;
    }
    break;
  case &Primop_e(Size,&List(e2,_)):
    switch (e2->r) {
    case &Var_e(_,&Global_b(v2)): fallthru(v2);
    case &Var_e(_,&Local_b(v2)):  fallthru(v2);
    case &Var_e(_,&Param_b(v2)):  fallthru(v2);
    case &Var_e(_,&Pat_b(v2)):
      if (v2->escapes) return r;
      return new List(new Reln(v,new LessEqSize(v2)),r);
    default: break;
    }
    break;
  default: break;
  }
  return r;
}

relns_t reln_assign_exp(relns_t r, exp_t e1, exp_t e2) {
  switch (e1->r) {
  case &Var_e(_,&Global_b(vd)): fallthru(vd);
  case &Var_e(_,&Param_b(vd)):  fallthru(vd);
  case &Var_e(_,&Local_b(vd)):  fallthru(vd);
  case &Var_e(_,&Pat_b(vd)):
    if (!vd->escapes)
      return reln_assign_var(r, vd, e2);
    break;
  default: break;
  }
  return r;
}

void print_reln(reln_t r) {
  fprintf(stderr,"%s",qvar2string(r->vd->name));
  switch (r->rop) {
  case &EqualConst(c): fprintf(stderr,"==%d",c); break;
  case &LessVar(vd):   fprintf(stderr,"<%s",qvar2string(vd->name)); break;
  case &LessSize(vd):  fprintf(stderr,"<%s.size",qvar2string(vd->name)); break;
  case &LessConst(c):  fprintf(stderr,"<%d",c); break;
  case &LessEqSize(vd): fprintf(stderr,"<=%s.size",qvar2string(vd->name));break;
  }
}

void print_relns(relns_t r) {
  for (; r != NULL; r = r->tl) {
    print_reln(r->hd);
    if (r->tl != NULL) fprintf(stderr,",");
  }
}

// returns true when t contains the region rgn with a few exceptions:
//  * doesn't count rgn in function types
//  * doesn't count rgn in region_t types (i.e., handles)
// We don't care about functions -- they're only callable if their 
// effects are covered by the capability and in the absence of closures,
// can't have any dangling pointers into rgn.  Region handles aren't
// "killed" when we reset a region and in fact we need them to ensure
// that we can allocate stuff in the region.  
static bool contains_region(tvar_t rgn, type_t t) {
  switch (Tcutil::compress(t)) {
  case VoidType: // these cases can't contain rgn
  case &IntType(_,_):    
  case FloatType:        
  case &DoubleType(_):   
  case &EnumType(_,_):   
  case &AnonEnumType(_): 
  case &SizeofType(_):   
  case &TagType(_):      
  case &TypeInt(_):      
  case HeapRgn:          
  case &Evar(_,_,_,_): return false; // because of compress above
  case &VarType(tv): return tvar_cmp(tv,rgn) == 0;
    // for parameterized types, we just look to see if rgn shows up 
    // in the parameters -- this is overly aggressive since the regions
    // might not actually get used or may only get used in a handle or
    // in a function.  But it beats substituting and descending.
    //
    // Note also that we don't descend into the definitions of [x]tunions,
    // structs, typedefs, etc. because they must be closed w.r.t. all regions
    // except the heap.  
  case &TunionType(TunionInfo{_,targs,r}): 
    if (contains_region(rgn,r)) return true;
    fallthru(targs);
  case &TypedefType(_,targs,_,_):    fallthru(targs);
  case &AggrType(AggrInfo{_,targs}): fallthru(targs);
  case &TunionFieldType(TunionFieldInfo{_,targs}): fallthru(targs);
  case &JoinEff(targs): 
    return List::exists_c(contains_region,rgn,targs);
  case &PointerType(PtrInfo{t2,_,PtrAtts(r,_,_,_)}):
    return contains_region(rgn,r) || contains_region(rgn,t2);
  case &FnType(_): return false;
  case &TupleType(tqs):
    for (; tqs != NULL; tqs = tqs->tl)
      if (contains_region(rgn,(*tqs->hd)[1])) return true;
    return false;
  case &AnonAggrType(_,fds):
    for (; fds != NULL; fds = fds->tl) 
      if (contains_region(rgn,fds->hd->type)) return true;
    return false;
  case &ArrayType(ArrayInfo{t2,_,_,_}): fallthru(t2);
  case &AccessEff(t2):     fallthru(t2);
  case &RgnsEff(t2):       return contains_region(rgn,t2);
  // careful: we don't want to kill any region handles to the region in
  // question.
  case &RgnHandleType(t2): return false;
  }
}

// used by kill_flowdict_region below
static void kill_root($(flowdict_t,tvar_t)@ env, root_t root, absRval_t rval) {
  let &$(*fd,rgn) = env;
  switch (root) {
  case &VarRoot(vd): 
    // if the variable's type contains the region
    if (contains_region(rgn,vd->type))
      // set it to uninitialized in the new dictionary
      rval = typ_to_absrval(vd->type, unknown_none);
    *fd = Dict::insert(*fd,root,rval);
    break;
  case &MallocPt(_,t): 
    // if the result of the malloc's type does not contain the region
    if (!contains_region(rgn,t))
      // insert the mapping into the new dictionary
      *fd = Dict::insert(*fd,root,rval);
    break;
  case &InitParam(_,_):
    // when we have interprocedural resettable, this could be a problem?!
    break;
  }
}

// "kill" all roots in the flowdict that have rgn in their type
static flowdict_t kill_flowdict_region(flowdict_t fd, type_t rgn) {
  tvar_t rgn_tvar;
  switch (Tcutil::compress(rgn)) {
  case &VarType(tv): rgn_tvar = tv; break;
  default: throw new Core::Impossible("kill_flowdict_region"); break;
  }
  $(flowdict_t,tvar_t) env = $(Dict::empty(root_cmp),rgn_tvar);
  Dict::iter_c(kill_root,&env,fd);
  return env[0];
}

// "kill" all relations in the flow analysis that involve the given region
static relns_t kill_relns_region(relns_t relns, type_t rgn) {
  // FIX: too conservative
  return NULL;
}

// the region rgn has been reset -- produce an outflow where the 
// roots involving rgn are now uninitialized.
flow_t kill_flow_region(flow_t f, type_t rgn) {
  switch (f) {
  case BottomFL: return f;
  case &ReachableFL(fd,r): 
    let fd2 = kill_flowdict_region(fd,rgn);
    let r2 = kill_relns_region(r,rgn);
    return new ReachableFL(fd2,r2);
  }
}
