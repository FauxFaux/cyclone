/* Utilities for control flow analysis.
   Copyright (C) 2001 Dan Grossman, Greg Morrisett
   This file is part of the Cyclone compiler.

   The Cyclone compiler is free software; you can redistribute it
   and/or modify it under the terms of the GNU General Public License
   as published by the Free Software Foundation; either version 2 of
   the License, or (at your option) any later version.

   The Cyclone compiler is distributed in the hope that it will be
   useful, but WITHOUT ANY WARRANTY; without even the implied warranty
   of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with the Cyclone compiler; see the file COPYING. If not,
   write to the Free Software Foundation, Inc., 59 Temple Place -
   Suite 330, Boston, MA 02111-1307, USA. */

#include <core.h>
#include <list.h>
#include <set.h>
#include <dict.h>
#include <cstring.h>
#include "absyn.h"
#include "tcutil.h"

#define CF_FLOWINFO_CYC
#include "cf_flowinfo.h"

using Absyn;
using List;
namespace CfFlowInfo;

////////////////////////// Shared Constants //////////////////////////
static tunion AbsRVal.UnknownR unknown_none_v = UnknownR(NoneIL);
static tunion AbsRVal.UnknownR unknown_this_v = UnknownR(ThisIL);
static tunion AbsRVal.UnknownR unknown_all_v  = UnknownR(AllIL);
static tunion AbsRVal.Esc      esc_none_v     = Esc(NoneIL);
static tunion AbsRVal.Esc      esc_this_v     = Esc(ThisIL);
static tunion AbsRVal.Esc      esc_all_v      = Esc(AllIL);
absRval_t unknown_none = &unknown_none_v;
absRval_t unknown_this = &unknown_this_v;
absRval_t unknown_all  = &unknown_all_v;
absRval_t esc_none     = &esc_none_v;
absRval_t esc_this     = &esc_this_v;
absRval_t esc_all      = &esc_all_v;

static place_set_t * mt_place_set_opt = NULL;
place_set_t mt_place_set() {
  if(mt_place_set_opt == NULL)
    mt_place_set_opt = new Set::empty(place_cmp);
  return *mt_place_set_opt;
}

//////////////////////////// Utilities ///////////////////////////////
int root_cmp(root_t r1, root_t r2) {
  if(((int)r1) == ((int)r2))
    return 0;
  int p1, p2;
  switch(r1) {case &VarRoot(x): p1=(int)x; break; case &MallocPt(x): p1=(int)x;}
  switch(r2) {case &VarRoot(x): p2=(int)x; break; case &MallocPt(x): p2=(int)x;}
  return p1 - p2;
}
int place_cmp(place_t p1, place_t p2) {
  if(((int)p1) == ((int)p2))
    return 0;
  int i = root_cmp(p1->root,p2->root);
  if(i != 0)
    return i;
  return list_cmp(Std::zstrptrcmp,p1->fields,p2->fields);
}

// for debugging -- for errors, convert e->loc to a string!
static mstringptr_t place2string(place_t p) {
  list_t<mstringptr_t> sl = NULL;
  switch(p->root) {
  case &VarRoot(vd): sl = new List(new aprintf("%s",*(*vd->name)[1]),sl); break;
  case &MallocPt(e): sl = new List(new aprintf("mpt%d",(int)e),sl);
  }
  for(_ fields = p->fields; fields != NULL; fields = fields->tl)
    sl = new List(new aprintf("%s", *fields->hd), sl);
  let ans = new aprintf("%s","");
  for(; sl != NULL; sl = sl->tl)
    *ans = aprintf("%s.%s", *sl->hd, *ans);
  return ans;
}

// Note: We don't have to worry about instantiating polymorphic struct fields
//       b/c they can't be instantiated with aggregates.
//   (If we ever have memory kinds, then we either need to figure out field's
//    instantiated types or not allow partial initialization of such fields.)
// Note: We could memoize answers here.
absRval_t typ_to_absrval(type_t t, absRval_t leafval) {
  switch(Tcutil::compress(t)) {
  case &TunionFieldType(TunionFieldInfo(&KnownTunionfield(_,tuf),_)):
    if(tuf->typs == NULL)
      return leafval;
    fallthru(tuf->typs);
  case &TupleType(tqts):
    aggrdict_t d = Dict::empty(Std::zstrptrcmp);
    for(int i=0; tqts != NULL; tqts = tqts->tl, ++i)
      d = Dict::insert(d, fieldname(i), typ_to_absrval((*tqts->hd)[1],leafval));
    return new Aggregate(d);

  case &AnonStructType(fs): fallthru(fs);
  case &StructType(_,_, & &Structdecl(_,_,_,&Core::Opt(fs),_)):
    aggrdict_t d = Dict::empty(Std::zstrptrcmp);
    for(; fs != NULL; fs = fs->tl) {
      let &Structfield(n,_,t2,_,_) = fs->hd;
      if((*n).size != 1) //empty name implies bit field so no need to include it
        d = Dict::insert(d, n, typ_to_absrval(t2,leafval));
    }
    return new Aggregate(d);

  default: return leafval;
  }
}

static bool prefix_of_member(place_t place, Set::set_t<place_t> set) {
  for(let elts = Set::elements(set); elts != NULL; elts = elts->tl) {
    let place2 = elts->hd;
    if(root_cmp(place->root, place2->root) != 0)
      continue;
    let fs1 = place->fields;
    let fs2 = place2->fields;
    for(; fs1 != NULL && fs2 != NULL; fs1 = fs1->tl, fs2 = fs2->tl)
      if(Std::zstrptrcmp(fs1->hd,fs2->hd) != 0)
	break;
    if(fs1 == NULL)
      return true;
  }
  return false;
}

//////////////////////// Managing Escapes ///////////////////////////

// Note: This would be a great place to delay an idempotent genregion
//       because we often won't have any places! (Also would want to pick
//       a very small initial region size.)
static struct EscPile<`r::R> {
  region_t<`r>       rgn;
  list_t<place_t,`r> places;
};
typedef struct EscPile<`r1> @`r2 escpile_t<`r1,`r2>;

static void add_place(escpile_t pile, place_t<`H,`H> place) {
  // we expect lots of inserts and short lists, so we don't bother to sort
  // we check for repeats to avoid a very unlikely exponential blowup
  if(!List::mem(place_cmp, pile->places, place))
    pile->places = rnew(pile->rgn) List(place, pile->places);
}
static void add_places(escpile_t pile, `a a, absRval_t r) {
  switch(r) {
  case &AddressOf(p): add_place(pile, p); return;
  case &Aggregate(d): Dict::iter_c(add_places, pile, d); return;
  default: return;
  }
}

// WARNING: do not use this to clobber with an Esc unless that's the point!
static absRval_t insert_place_inner(absRval_t new_val,absRval_t old_val) {
  switch(old_val) {
  case &Aggregate(d): return new Aggregate(Dict::map_c(insert_place_inner,
						       new_val, d));
  default: return new_val;
  }
}
static absRval_t insert_place_outer(list_t<field_name_t> fs,
				    absRval_t old_val, absRval_t new_val) {
  if(fs==NULL) 
    return insert_place_inner(new_val,old_val);
  switch($(fs,old_val)) {
  case $(&List(hd,tl),&Aggregate(d)):
    let new_child = insert_place_outer(tl,Dict::lookup(d,hd), new_val);
    return new Aggregate(Dict::insert(d, hd, new_child));
  default: throw new Core::Impossible("bad insert place");
  }
}
// Note: this terminates because we thread the dictionary.
// Note: we need to catch not in the dict b/c of how join_flow uses this code?
// Note: we should only be escaping leaves now -- never Aggregate!
// Note: There's a quadratic behavior here that won't be a problem in practice.
static flowdict_t escape_these(escpile_t pile, place_set_t * all_changed,
			       flowdict_t d) {
  while(pile->places != NULL) {
    let place = pile->places->hd;
    pile->places = pile->places->tl;
    if(all_changed != NULL)
      *all_changed = Set::insert(*all_changed, place);
    absRval_t oldval, newval;
    try oldval = lookup_place(d,place); 
    catch { case Dict::Absent: continue; }
    switch(initlevel(d,oldval)) {
    case AllIL:  newval = esc_all;  break;
    case ThisIL: newval = esc_this; break;
    case NoneIL: newval = esc_none; break;
    }
    add_places(pile, 0, oldval);
    d = Dict::insert(d,place->root,
		     insert_place_outer(place->fields,
					Dict::lookup(d,place->root),
					newval));
  }
  return d;
}

////////////////////////// Flow Gets and (functional) Sets ///////////////////

// We must detect cycles in the MustPointTo graph or we won't terminate.
// We heap-allocate the seen list b/c we expect it to be very very short.
static struct InitlevelEnv {
  flowdict_t d;
  list_t<place_t> seen;
};
static initlevel_t initlevel_approx(absRval_t r) {
  switch(r) {
  case &UnknownR(il): return il;
  case &Esc(il):      return il;
  case Zero:
  case NotZeroAll:    return AllIL;
  case NotZeroThis:   return ThisIL;
  default: throw new Core::Impossible("initlevel_approx");
  }
}
static initlevel_t initlevel_rec(struct InitlevelEnv @ env, `a a, 
				 absRval_t r, initlevel_t acc) {
  initlevel_t this_ans;
  if(acc == NoneIL) return NoneIL;
  switch(r) {
  case &Aggregate(d): this_ans = Dict::fold_c(initlevel_rec,env,d,AllIL); break;
  case &AddressOf(p): 
    if(List::mem(place_cmp, env->seen, p))
      this_ans = AllIL;
    else {
      env->seen = new List(p, env->seen);
      this_ans  = initlevel_rec(env,0,lookup_place(env->d,p),AllIL);
      env->seen = env->seen->tl;
      if(this_ans == NoneIL)
	this_ans = ThisIL;
    }
    break;
  default: this_ans = initlevel_approx(r);
  }
  if(this_ans == NoneIL) return NoneIL;
  if(this_ans == ThisIL || acc == ThisIL) return ThisIL;
  return AllIL;
}
initlevel_t initlevel(flowdict_t d, absRval_t r) {
  let env = InitlevelEnv(d,NULL);
  return initlevel_rec(&env,0,r,AllIL);
}

absRval_t lookup_place(flowdict_t d, place_t place) {
  let &Place(root,fields) = place;
  let ans = Dict::lookup(d,root);
  for(; fields != NULL; fields = fields->tl)
    switch($(ans,fields->hd)) {
    case $(&Aggregate(d2), fname): ans = Dict::lookup(d2,fname); break;
    default: throw new Core::Impossible("bad lookup_place");
    }
  return ans;
}

static bool is_rval_unescaped(`a a, `b b, absRval_t rval) {
  switch(rval) {
  case &Esc(_):       return false;
  case &Aggregate(d): return Dict::forall_c(is_rval_unescaped,0,d);
  default:            return true;
  }
}
bool is_unescaped(flowdict_t d, place_t place) {
  return is_rval_unescaped(0,0,lookup_place(d,place));
}

// only things pointed to escape
flowdict_t escape_deref(flowdict_t d, place_set_t * all_changed, absRval_t r) {
  region rgn {
    escpile_t pile = rnew(rgn) EscPile(rgn,NULL);
    add_places(pile, 0, r);
    return escape_these(pile, all_changed, d);
  }
}

static struct AssignEnv<`r::R> {
  escpile_t<`r,`r> pile;
  flowdict_t       d;
  Position::seg_t  loc;
};
static absRval_t assign_place_inner(struct AssignEnv<`r> @ env, `a a,
				    absRval_t oldval, absRval_t newval) {
  // assignments to escaped places must be fully-init because other aliases
  // might assume fully-init.  Always causes place to be added for same reason.
  switch($(oldval,newval)) {
  case $(&Esc(_), &AddressOf(p)): add_place(env->pile,p); fallthru;
  case $(&Esc(_), _):
    if(initlevel(env->d,newval) != AllIL)
      Tcutil::terr(env->loc, "assignment puts possibly-uninitialized data in "
		   "an escaped location");
    return esc_all;
  case $(&Aggregate(d1), &Aggregate(d2)):
    // re-ordering dicts may reduce allocation?
    let new_d = Dict::union_two_c(assign_place_inner, env, d1, d2);
    if(new_d == d1) return oldval;
    if(new_d == d2) return newval;
    return new Aggregate(new_d);
  case $(_, &Esc(il)): // we already know we don't have an escaped location
    switch(il) {
    case NoneIL: return unknown_none;
    case ThisIL: return unknown_this;
    case AllIL:  return unknown_all;
    }
  default: return newval;
  }
}
static absRval_t assign_place_outer(struct AssignEnv<`r> @ env, 
				    list_t<field_name_t> fs,
				    absRval_t oldval, absRval_t newval) {
  if(fs == NULL) return assign_place_inner(env,0,oldval,newval);
  switch($(fs,oldval)) {
  case $(&List(hd,tl),&Aggregate(d)):
    let new_child = assign_place_outer(env,tl,Dict::lookup(d,hd),newval);
    return new Aggregate(Dict::insert(d, hd, new_child));
  default: throw new Core::Impossible("bad insert place");
  }
}
flowdict_t assign_place(Position::seg_t loc, flowdict_t d,
			place_set_t * all_changed, place_t<`H,`H> place, 
			absRval_t r) {
  if(all_changed != NULL)
    *all_changed = Set::insert(*all_changed,place);
  region rgn {
    let &Place(root,fs) = place;
    struct AssignEnv<`rgn> env = AssignEnv(rnew(rgn) EscPile(rgn,NULL), d, loc);
    absRval_t newval= assign_place_outer(&env,fs,Dict::lookup(d,root),r);
    return escape_these(env.pile, all_changed, Dict::insert(d, root, newval));
  }
}

/////////////////// Join, After, and Lessthan ///////////////////////////////
static struct JoinEnv<`r::R> {
  escpile_t<`r,`r> pile;
  flowdict_t       d1;
  flowdict_t       d2;
};
typedef struct JoinEnv<`r1> @`r2 joinenv_t<`r1,`r2>;
static enum WhoIsChanged { Neither, One, Two }; // if both changed, take join
static struct AfterEnv<`r::R> {
  struct JoinEnv<`r> joinenv;
  Set::set_t<place_t> chg1; // should region-allocate!
  Set::set_t<place_t> chg2; // should region-allocate!
  place_t curr_place; // a pain to stack-allocate, but should!
  list_t<field_name_t> @ last_field_cell;
  enum WhoIsChanged changed;
};
typedef struct AfterEnv<`r1> @`r2 afterenv_t<`r1,`r2>;

// note: There is another reasonable policy for losing an alias to AllInit:
//   (when destination unescaped!)
//   still make the result ThisInit and **don't** lose the aliases downstream.
//   I imagine the default should be AllInit and lose downstream with a pragma
//   in the source to get the other behavior (if anyone actually cares).
//   Note that the two policies are incomparable in what they accept.
// note: we don't have to record changes because anything changed would
//       have to have already been added as a change by r1 or r2!
static absRval_t join_absRval(joinenv_t env,`a a,absRval_t r1,absRval_t r2) {
  if(r1==r2) return r1;

  switch($(r1,r2)) { // only break when the answer should be Unknown or Esc!

  case $(&AddressOf(p1), &AddressOf(p2)):
    if(place_cmp(p1,p2)==0) return r1;
    add_place(env->pile,p1);
    add_place(env->pile,p2);
    break;
  case $(&AddressOf(p), _): add_place(env->pile,p); break;
  case $(_, &AddressOf(p)): add_place(env->pile,p); break;

  case $(NotZeroAll,  NotZeroThis):
  case $(NotZeroThis, NotZeroAll): return NotZeroThis;

  case $(&Aggregate(d1), &Aggregate(d2)):
    let new_d = Dict::union_two_c(join_absRval, env, d1, d2);
    if(new_d == d1) return r1;
    if(new_d == d2) return r2;
    return new Aggregate(new_d);
    
  default: break;
  }
  initlevel_t il1 = initlevel(env->d1,r1);
  initlevel_t il2 = initlevel(env->d2,r2);
  switch($(r1,r2)) {
  case $(&Esc(_),_):
  case $(_,&Esc(_)):
    switch($(il1,il2)) {
    case $(_,NoneIL): case $(NoneIL,_): return esc_none;
    case $(_,ThisIL): case $(ThisIL,_): return esc_this;
    default: return esc_all;
    }
  default:
    switch($(il1,il2)) {
    case $(_,NoneIL): case $(NoneIL,_): return unknown_none;
    case $(_,ThisIL): case $(ThisIL,_): return unknown_this;
    default: return unknown_all;
    }
  }
}
flow_t join_flow(place_set_t* all_changed, flow_t f1, flow_t f2){
  if(f1 == f2) return f1;
  switch($(f1,f2)) {
  case $(BottomFL,_): return f2;
  case $(_,BottomFL): return f1;
  case $(&ReachableFL(d1),&ReachableFL(d2)):
    // A lot of computation to avoid allocation, but profiling suggests
    // that's the right thing to do.
    if(d1 == d2) return f1;
    if(flow_lessthan_approx(f1,f2)) return f2;
    if(flow_lessthan_approx(f2,f1)) return f1;
    region rgn {
      struct JoinEnv<`rgn> env = JoinEnv(rnew(rgn) EscPile(rgn,NULL),d1,d2);
      flowdict_t outdict = Dict::intersect_c(join_absRval,&env,d1,d2);
      return new ReachableFL(escape_these(env.pile, all_changed, outdict));
    }
  }
}

static absRval_t after_absRval_d(afterenv_t,field_name_t,absRval_t,absRval_t);
static absRval_t after_absRval(afterenv_t env, absRval_t r1, absRval_t r2) {
  let changed1 = env->changed == One || Set::member(env->chg1,env->curr_place);
  let changed2 = env->changed == Two || Set::member(env->chg2,env->curr_place);
  if(changed1 && changed2)
    return join_absRval(&env->joinenv, 0, r1, r2);
  // checking for "is a contained object changed in the other pinfo" is
  // quadratic because we'll do it "all the way down", but in practice
  // "all the way down" is small.
  if(changed1) {
    if(!prefix_of_member(env->curr_place, env->chg2))
      return r1;
    env->changed = One;
  }
  if(changed2) {
    if(!prefix_of_member(env->curr_place, env->chg1))
      return r2;
    env->changed = Two;
  }
  switch($(r1,r2)) {
  case $(&Aggregate(d1),&Aggregate(d2)):
    let new_d = Dict::union_two_c(after_absRval_d,env,d1,d2);
    if(new_d == d1) return r1;
    if(new_d == d2) return r2;
    return new Aggregate(new_d);
  default: throw new Core::Impossible("after_pathinfo -- non-aggregates!");
  }
}
static absRval_t after_absRval_d(afterenv_t env, field_name_t key,
				 absRval_t r1, absRval_t r2) {
  if(r1 == r2) return r1;
  let old_last_field = env->last_field_cell; 
  let old_changed    = env->changed;
  *env->last_field_cell = new List(key,NULL);
  env->last_field_cell  = &(*env->last_field_cell)->tl;
  let ans = after_absRval(env,r1,r2);
  env->last_field_cell        = old_last_field;
  (*env->last_field_cell)->tl = NULL;
  env->changed                = old_changed;
  return ans;
}
static absRval_t after_root(afterenv_t env, root_t root,
			    absRval_t r1, absRval_t r2) {
  if(r1 == r2) return r1;
  *env->curr_place     = Place(root,NULL);
  env->last_field_cell = &env->curr_place->fields;
  env->changed         = Neither;
  return after_absRval(env, r1, r2);
}
// FIX: region-allocate chg1 and chg2
flow_t after_flow(place_set_t* all_changed, flow_t f1, flow_t f2,
		  place_set_t chg1, place_set_t chg2) {
  static tunion Raw_exp.Const_e dummy_rawexp = Const_e(Null_c);
  static struct Exp             dummy_exp    = Exp(NULL,&dummy_rawexp,NULL);
  static tunion Root.MallocPt   dummy_root   = MallocPt(&dummy_exp);

  if(f1 == f2) return f1;
  switch($(f1,f2)) {
  case $(BottomFL,_): 
  case $(_,BottomFL): return BottomFL;
  case $(&ReachableFL(d1),&ReachableFL(d2)):
    if(d1 == d2) return f1;
    // Should think about ways to avoid allocation here: (curr_place and
    // cut-off in presence of changes)
    region rgn {
      let curr_place = new Place(&dummy_root,NULL);
      let env  = AfterEnv(JoinEnv(rnew(rgn) EscPile(rgn,NULL), d1, d2), 
			  chg1, chg2, 
			  curr_place, &curr_place->fields,
			  Neither);
      let new_d = Dict::union_two_c(after_root,&env,d1,d2);
      return new ReachableFL(escape_these(env.joinenv.pile,all_changed,new_d));
    }
  }
}

// WARNING: This is where we cheat ("approx") for performance.
static bool absRval_lessthan_approx(`a ignore, absRval_t r1, absRval_t r2) {
  if(r1 == r2) return true;

  switch ($(r1,r2)) {
  case $(&AddressOf(p1), &AddressOf(p2)): return place_cmp(p1,p2)==0;
  case $(&AddressOf(_),_):
  case $(_,&AddressOf(_)): return false;
  case $(&Aggregate(d1), &Aggregate(d2)):
    return (d1 == d2) || Dict::forall_intersect(absRval_lessthan_approx,d1,d2);
  case $(_, NotZeroThis): return r1==NotZeroAll;
  case $(_, Zero): 
  case $(_, NotZeroAll): return false;
  case $(&Esc(_),&Esc(_)): break;
  case $(&Esc(_),_): return false;
    /*  case $(&Aggregate(_),_):
	case $(_,&Aggregate(_)): fprintf(stderr,"ACK!"); */
  default: break;
  }
  switch($(initlevel_approx(r1), initlevel_approx(r2))) {
  case $(AllIL,AllIL): return true;
  case $(_,NoneIL): return true;
  case $(NoneIL,_): return false;
  case $(_,ThisIL): return true;
  case $(ThisIL,_): return false;
  }
}

// WARNING: We assume anything not in the intersection is IRRELEVANT.
// WARNING: We might return false even when f1 < f2, but that's okay b/c
//  that will just cause clients to take joins/afters and we always return true
//  when the two flows are equal.  That's what the approx means.
// (As a result of the approx, we don't have to allocate here.)
bool flow_lessthan_approx(flow_t f1, flow_t f2) {
  if(f1 == f2) return true;
  switch($(f1,f2)) {
  case $(BottomFL,_): return true;
  case $(_,BottomFL): return false;
  case $(&ReachableFL(d1),&ReachableFL(d2)):
    if(d1 == d2) return true;
    return Dict::forall_intersect(absRval_lessthan_approx,d1,d2);
  }
}
